{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e77b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join as pjoin\n",
    "from transformers import CLIPModel, CLIPTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence \n",
    "from tqdm import tqdm\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import PoseTransformer\n",
    "# from PoseTransformer import PoseTransformer, CrossModalTransformer\n",
    "from NoiseScheduler import NoiseScheduler\n",
    "from utils.motion_process import recover_from_ric\n",
    "import importlib\n",
    "\n",
    "mp.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "ACCUMULATION_STEPS = 4\n",
    "EPOCHS = 7\n",
    "SAVE_PATH = \"model_output\"\n",
    "\n",
    "LEARNING_RATE = 3e-5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "PERCENT_TRAIN = .8\n",
    "\n",
    "EMBEDDING_DIM = 512 \n",
    "POSE_FEATURES_DIM = 263"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6cbdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = \"..\\\\HumanML3D\"\n",
    "train_list = open(pjoin(src_dir, \"train.txt\"), \"r\", encoding=\"utf-8\")\n",
    "val_list = open(pjoin(src_dir, \"val.txt\"), \"r\", encoding=\"utf-8\")\n",
    "test_list = open(pjoin(src_dir, \"test.txt\"), \"r\", encoding=\"utf-8\")\n",
    "\n",
    "clip_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa87a2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_embedding(x, num_frequencies=10):\n",
    "    \"\"\"\n",
    "    Applies Fourier feature encoding to the input tensor.\n",
    "    x: (seq_len, 3) - The trajectory data (X, Y, Z)\n",
    "    num_frequencies: The number of frequency bands\n",
    "    \"\"\"\n",
    "    seq_len, dim = x.shape  # dim = 3 (X, Y, Z)\n",
    "    \n",
    "    # Define frequency bands: log-spaced\n",
    "    freqs = torch.logspace(0.0, np.log10(1000.0), num_frequencies)  # Frequencies in range [1, 1000]\n",
    "    \n",
    "    # Compute sin and cos embeddings for each frequency\n",
    "    x_proj = x.unsqueeze(-1) * freqs  # Shape: (seq_len, 3, num_frequencies)\n",
    "    fourier_features = torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)  # (seq_len, 3, 2*num_frequencies)\n",
    "    \n",
    "    return fourier_features.view(seq_len, -1)  # Flatten to (seq_len, 3 * 2 * num_frequencies)\n",
    "\n",
    "class PoseTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "        (data_dir): contains all the new_joint_vecs\n",
    "        (train_list): the train.txt, as an opened file\n",
    "        (max_len): max length of text descriptions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, src_dir, setting, tokenizer, joint_num, max_len=256, use_percentage = 1.0):\n",
    "        self.src_dir = src_dir\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.joint_num = joint_num \n",
    "        self.pose_features_dim = POSE_FEATURES_DIM\n",
    "\n",
    "        if setting not in [\"train\", \"train_temp\", \"val\", \"test\", \"all\"]:\n",
    "            print(\"Invalid setting. Must be train, val, test, or all.\")\n",
    "            raise\n",
    "\n",
    "        with open(pjoin(src_dir, f\"{setting}.txt\"), \"r\") as f:\n",
    "            self.file_list = [line.strip() for line in f.readlines()]\n",
    "\n",
    "        if 0 < use_percentage < 1.0:\n",
    "            random.shuffle(self.file_list)\n",
    "            num_samples = int(len(self.file_list) * use_percentage)\n",
    "            self.file_list = self.file_list[:num_samples]\n",
    "\n",
    "        # get stats to normalize data later on\n",
    "        self.mean, self.std = None, None\n",
    "        stats = np.load(\"pose_stats.npz\")\n",
    "        self.mean = torch.tensor(stats['mean'], dtype=torch.float32).unsqueeze(0) # Shape (1, pose_features_dim)\n",
    "        self.std = torch.tensor(stats['std'], dtype=torch.float32).unsqueeze(0)   # Shape (1, pose_features_dim)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        file = self.file_list[index]\n",
    "\n",
    "        # Load pose data\n",
    "        pose_path = pjoin(self.src_dir, \"new_joint_vecs\", f\"{file}.npy\")\n",
    "        pose_data = np.load(pose_path)  # Shape: (frames, joints, features)\n",
    "        pose_tensor = torch.tensor(pose_data, dtype=torch.float32)\n",
    "        # normalize\n",
    "        pose_tensor_normalized = (pose_tensor - self.mean) / self.std\n",
    "\n",
    "        # Load text description\n",
    "        text_path = pjoin(self.src_dir, \"texts\", f\"{file}.txt\")\n",
    "\n",
    "        # The descriptions have a extra information such as time stamp and part of speech. I will get rid of that for now to keep things simple.\n",
    "        with open(text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text_descriptions = [\n",
    "                re.split('#', line.strip())[0] for line in f.readlines() if line.strip()\n",
    "            ]\n",
    "\n",
    "        encoded_texts = self.tokenizer(\n",
    "            text_descriptions,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Since tokenizer returns tensors in a dictionary, we can access them directly\n",
    "        input_ids = encoded_texts[\"input_ids\"]\n",
    "        attention_mask = encoded_texts[\"attention_mask\"]\n",
    "\n",
    "        world_joints = recover_from_ric(torch.from_numpy(pose_data).float(), self.joint_num)\n",
    "        root_positions = world_joints[:, 0, :] # get trajectory from root position\n",
    "        fourier_encoded_traj = fourier_embedding(root_positions)\n",
    "\n",
    "        # Return a list of dictionaries, one per description\n",
    "        return [{\n",
    "            \"pose\": pose_tensor_normalized,\n",
    "            \"text\": input_ids[i],\n",
    "            \"attention_mask\": attention_mask[i],\n",
    "            \"trajectory\": fourier_encoded_traj\n",
    "        } for i in range(len(text_descriptions))]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Pads all pose sequences to the maximum length in the batch.\n",
    "    \"\"\"\n",
    "    flattened_batch = [item for sublist in batch for item in sublist]\n",
    "    \n",
    "    # Extract poses and text descriptions from the batch\n",
    "    poses = [item[\"pose\"] for item in flattened_batch]  # List of pose tensors\n",
    "    texts = [item[\"text\"] for item in flattened_batch]  # List of tokenized text tensors\n",
    "    attention_masks = [item.get(\"attention_mask\", None) for item in flattened_batch]\n",
    "    trajectories = [item[\"trajectory\"] for item in flattened_batch]  # Fourier-encoded trajectories (varied lengths)\n",
    "\n",
    "    # Pad pose sequences (pad with zeros to max length in the batch)\n",
    "    padded_poses = pad_sequence(poses, batch_first=True, padding_value=0.0)  # Shape: (batch_size, max_len, ...)\n",
    "    pose_mask = (padded_poses.sum(dim=-1) != 0).float()  # Sum across features, Shape: (batch_size, max_pose_len)\n",
    "\n",
    "    # Pad trajectory sequences (pad with zeros to max length in the batch)\n",
    "    padded_trajectories = pad_sequence(trajectories, batch_first=True, padding_value=0.0)  # Shape: (batch_size, max_traj_len, fourier_dim)\n",
    "\n",
    "    # Stack text tensors\n",
    "    padded_texts = torch.stack(texts, dim=0)\n",
    "    padded_attention_masks = torch.stack(attention_masks, dim=0)\n",
    "\n",
    "    return {\n",
    "        \"pose\": padded_poses,\n",
    "        \"pose_mask\": pose_mask,\n",
    "        \"text\": padded_texts,\n",
    "        \"attention_mask\": padded_attention_masks,\n",
    "        \"trajectory\": padded_trajectories,\n",
    "        # trajectory_mask not needed as it shares the same dimensions as pose_mask\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c460be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, options, checkpoint_path = None):\n",
    "        self.device = options['device']\n",
    "        self.train_dataloader = options['train_dataloader']\n",
    "        self.eval_dataloader = options['eval_dataloader']\n",
    "        self.clip_model = options['clip_model'].to(self.device)\n",
    "        self.clip_tokenizer = options['clip_tokenizer']\n",
    "        self.pose_transformer = options['pose_transformer'].to(self.device)\n",
    "        self.text_cross_transformer = options['text_cross_transformer'].to(self.device)\n",
    "        # self.trajectory_cross_transformer = options['trajectory_cross_transformer'].to(self.device)\n",
    "        self.pose_features_dim = options['pose_features_dim']\n",
    "        self.noise_predictor = options['noise_predictor'].to(self.device)\n",
    "        self.accumulation_steps = options['accumulation_steps']\n",
    "\n",
    "        self.noise_scheduler = NoiseScheduler(timesteps=1000)\n",
    "        # clip is left out of the optimizer, as we won't be tuning CLIP model\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            list(self.pose_transformer.parameters()) +\n",
    "            list(self.text_cross_transformer.parameters()) +\n",
    "            # list(self.trajectory_cross_transformer.parameters()) +\n",
    "            list(self.noise_predictor.parameters()),\n",
    "            lr=LEARNING_RATE,\n",
    "            weight_decay=WEIGHT_DECAY\n",
    "        )\n",
    "\n",
    "        # num_batches = len(self.train_dataloader) // ACCUMULATION_STEPS\n",
    "        # # Add 1 if there are leftover batches that will trigger an optimizer step\n",
    "        # if len(self.train_dataloader) % ACCUMULATION_STEPS != 0:\n",
    "        #     num_batches += 1\n",
    "        # training_steps = num_batches * EPOCHS\n",
    "        # warmup_steps = int(training_steps * 0.05)\n",
    "        # self.lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "        #     self.optimizer,\n",
    "        #     num_warmup_steps=warmup_steps,  # Gradually increase LR\n",
    "        #     num_training_steps=training_steps  # Cosine decay over training\n",
    "        # )\n",
    "\n",
    "        self.pose_transformer.train()\n",
    "        self.text_cross_transformer.train()\n",
    "        # self.trajectory_cross_transformer.train()\n",
    "        self.noise_predictor.train()\n",
    "\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "\n",
    "    def save_model(self):\n",
    "        \"\"\"Save only the model weights without optimizer state.\"\"\"\n",
    "        state = {\n",
    "            \"pose_transformer\": self.pose_transformer.state_dict(),\n",
    "            \"text_cross_transformer\": self.text_cross_transformer.state_dict(),\n",
    "            \"noise_predictor\": self.noise_predictor.state_dict(),\n",
    "        }\n",
    "        torch.save(state, self.checkpoint_path)\n",
    "        print(f\"Model weights saved to {self.checkpoint_path}\")\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Load model weights if a checkpoint exists, ignoring optimizer.\"\"\"\n",
    "        if os.path.exists(self.checkpoint_path):\n",
    "            checkpoint = torch.load(self.checkpoint_path, map_location=self.device)\n",
    "            self.pose_transformer.load_state_dict(checkpoint[\"pose_transformer\"])\n",
    "            self.text_cross_transformer.load_state_dict(checkpoint[\"text_cross_transformer\"])\n",
    "            self.noise_predictor.load_state_dict(checkpoint[\"noise_predictor\"])\n",
    "            print(f\"Model weights loaded from {self.checkpoint_path}\")\n",
    "\n",
    "    def get_lr_lambda(self):\n",
    "        return lambda step: min((step + 1) / self.warmup_steps, 1.0)\n",
    "    \n",
    "    def _process_batch(self, batch):\n",
    "        poses = batch[\"pose\"].to(self.device)\n",
    "        pose_mask = batch[\"pose_mask\"].to(self.device)\n",
    "        texts = batch[\"text\"].to(self.device)\n",
    "        text_mask = batch[\"attention_mask\"].to(self.device)\n",
    "        # trajectory = batch[\"trajectory\"].to(self.device)\n",
    "\n",
    "        # Denoiser setup\n",
    "        print(f\"process batch, pose shape: {poses.shape}\")\n",
    "        batch_size = poses.shape[0]\n",
    "        noise = torch.randn_like(poses)\n",
    "        timesteps = self.noise_scheduler.sample_timesteps(batch_size, device=self.device)\n",
    "        noisy_poses = self.noise_scheduler.add_noise(poses, noise, timesteps)\n",
    "\n",
    "        # Get text embeddings from CLIP\n",
    "        with torch.no_grad(): # Ensure no gradients computed for CLIP\n",
    "            text_embeddings = self.clip_model.get_text_features(input_ids=texts, attention_mask=text_mask)\n",
    "        # print(f\"Noisy poses dimensions: {noisy_poses.shape}\")\n",
    "\n",
    "        pose_embeddings = self.pose_transformer(\n",
    "            noisy_poses,\n",
    "            pose_mask=pose_mask,\n",
    "            timesteps=timesteps\n",
    "        )\n",
    "\n",
    "        # trajectory_conditioned_embeddings = self.trajectory_cross_transformer(\n",
    "        #     pose_embeddings,\n",
    "        #     trajectory,\n",
    "        #     pose_mask=pose_mask,\n",
    "        #     memory_mask=pose_mask\n",
    "        # )\n",
    "\n",
    "        # Cross-attention with text embeddings\n",
    "        text_conditioned_embeddings = self.text_cross_transformer(\n",
    "            pose_embeddings, # trajectory_conditioned_embeddings,\n",
    "            text_embeddings,\n",
    "            pose_mask=pose_mask,\n",
    "            memory_mask=None\n",
    "        )\n",
    "        # Predict noise\n",
    "        predicted_noise = self.noise_predictor(text_conditioned_embeddings)\n",
    "        # print(f\"Predicted noise dimensions: {noisy_poses.shape}\")\n",
    "\n",
    "        return predicted_noise, noise\n",
    "\n",
    "    def train(self):\n",
    "        dataloader = self.train_dataloader\n",
    "        self.pose_transformer.train()\n",
    "        self.text_cross_transformer.train()\n",
    "        # self.trajectory_cross_transformer.train()\n",
    "        self.noise_predictor.train()\n",
    "\n",
    "        total_loss = 0\n",
    "        num_batches = len(dataloader)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        for i, batch in enumerate(tqdm(dataloader, leave=True)):\n",
    "            predicted_noise, noise = self._process_batch(batch)\n",
    "\n",
    "            # Compute MSE loss\n",
    "            loss = torch.nn.functional.mse_loss(predicted_noise, noise)\n",
    "            loss = loss / self.accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            if (i + 1) % self.accumulation_steps == 0 or (i + 1) == num_batches: #accumulate gradients, and then update.\n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(self.pose_transformer.parameters(), 1.0)\n",
    "                torch.nn.utils.clip_grad_norm_(self.text_cross_transformer.parameters(), 1.0)\n",
    "                torch.nn.utils.clip_grad_norm_(self.noise_predictor.parameters(), 1.0)\n",
    "\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                # self.lr_scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        return total_loss / num_batches\n",
    "\n",
    "    def eval(self):\n",
    "        dataloader = self.eval_dataloader\n",
    "        self.pose_transformer.eval()\n",
    "        self.text_cross_transformer.eval()\n",
    "        # self.trajectory_cross_transformer.eval()\n",
    "        self.noise_predictor.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            for batch in tqdm(dataloader, leave=True):\n",
    "                predicted_noise, noise = self._process_batch(batch)\n",
    "\n",
    "                # Compute MSE loss\n",
    "                loss = torch.nn.functional.mse_loss(predicted_noise, noise)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        return total_loss / len(dataloader)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def init_weights_transformer(module):\n",
    "    \"\"\"\n",
    "    Initialize weights using Transformer-style N(0, 0.02) for Linear & Embedding.\n",
    "    Initialize LayerNorm weights to 1 and biases to 0.\n",
    "    \"\"\"\n",
    "    std_dev = 0.02 # Standard deviation for normal initialization\n",
    "    if isinstance(module, nn.Linear):\n",
    "        # Normal initialization for weights\n",
    "        nn.init.normal_(module.weight, mean=0.0, std=std_dev)\n",
    "        # Zero initialization for biases if they exist\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "        # Normal initialization for embedding weights\n",
    "        nn.init.normal_(module.weight, mean=0.0, std=std_dev)\n",
    "        # Zero out padding embedding if it exists\n",
    "        if module.padding_idx is not None:\n",
    "            with torch.no_grad(): # Ensure this operation isn't tracked by autograd\n",
    "                module.weight[module.padding_idx].fill_(0)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        # Initialize weights (gamma) to 1\n",
    "        nn.init.ones_(module.weight)\n",
    "        # Initialize biases (beta) to 0\n",
    "        nn.init.zeros_(module.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419052c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Train dataset\n",
      "Loaded normalization stats for train_temp dataset.\n",
      "Loading Eval Dataset\n",
      "Loaded normalization stats for val dataset.\n",
      "Dataset loading done\n",
      "1000\n",
      "Pose Transformer Parameters: 29018624\n",
      "Text-Cross Transformer Parameters: 38361600\n",
      "Noise Predictor Parameters: 134919\n",
      "Total Trainable Parameters: 67515143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At get item: input id shape: torch.Size([4, 77])\n",
      "At get item: input id shape: torch.Size([4, 77])\n",
      "At get item: input id shape: torch.Size([4, 77])\n",
      "At get item: input id shape: torch.Size([4, 77])\n",
      "At get item: input id shape: torch.Size([4, 77])\n",
      "At get item: input id shape: torch.Size([4, 77])\n",
      "At get item: input id shape: torch.Size([4, 77])\n",
      "At get item: input id shape: torch.Size([3, 77])\n",
      "process batch, pose shape: torch.Size([31, 199, 263])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 87\u001b[0m\n\u001b[0;32m     85\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m---> 87\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m     89\u001b[0m     eval_loss \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[1;32mIn[8], line 135\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, optimizer)\u001b[0m\n\u001b[0;32m    133\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmse_loss(predicted_noise, noise)\n\u001b[0;32m    134\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulation_steps\n\u001b[1;32m--> 135\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m num_batches: \u001b[38;5;66;03m#accumulate gradients, and then update.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;66;03m# Gradient clipping\u001b[39;00m\n\u001b[0;32m    139\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpose_transformer\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kerwi\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kerwi\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kerwi\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)  # Python random module\n",
    "np.random.seed(SEED)  # NumPy random\n",
    "torch.manual_seed(SEED)  # PyTorch random\n",
    "\n",
    "# importlib.reload(PoseTransformer)\n",
    "\n",
    "checkpoint_path = f\"model_saves/Acc_TT_{PERCENT_TRAIN}_percentdata_lr{LEARNING_RATE}_wd{WEIGHT_DECAY}\"\n",
    "\n",
    "print(\"Loading Train dataset\")\n",
    "trainDataset = PoseTextDataset(src_dir=src_dir,setting=\"train_temp\",tokenizer=clip_tokenizer, joint_num=22, max_len=77,use_percentage=1)\n",
    "trainDataLoader = DataLoader(trainDataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=0,collate_fn=collate_fn)\n",
    "# persistent_workers=True,\n",
    "# multiprocessing_context=\"spawn\"\n",
    "print(\"Loading Eval Dataset\")\n",
    "evalDataset = PoseTextDataset(src_dir=src_dir,setting=\"val\",tokenizer=clip_tokenizer, joint_num=22, max_len=77,use_percentage=.01)\n",
    "evalDataLoader = DataLoader(evalDataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=0,collate_fn=collate_fn)\n",
    "print(\"Dataset loading done\")\n",
    "pose_transformer = PoseTransformer.PoseTransformer(\n",
    "    pose_dim=POSE_FEATURES_DIM,\n",
    "    embedding_dim=EMBEDDING_DIM,  # Match CLIP's embedding dimension\n",
    "    num_heads=8,  # Number of attention heads\n",
    "    num_layers=8,  # Number of transformer layers\n",
    "    dropout=0.1,  # Dropout probability,\n",
    "    use_decoder=False\n",
    ")\n",
    "\n",
    "text_cross_transformer = PoseTransformer.CrossModalTransformer(\n",
    "    pose_dim = EMBEDDING_DIM,\n",
    "    memory_dim= EMBEDDING_DIM,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    num_heads=8,\n",
    "    num_layers=8,\n",
    "    use_decoder=True\n",
    ")\n",
    "\n",
    "noise_predictor = nn.Sequential(\n",
    "        nn.Linear(EMBEDDING_DIM, EMBEDDING_DIM), # Keep dim or expand\n",
    "        nn.SiLU(),                              # Activation (SiLU/Swish)\n",
    "        nn.Linear(EMBEDDING_DIM, POSE_FEATURES_DIM) # Project to output dim\n",
    "    )\n",
    "\n",
    "pose_transformer.apply(init_weights_transformer)\n",
    "text_cross_transformer.apply(init_weights_transformer)\n",
    "noise_predictor.apply(init_weights_transformer) # Apply standard init first\n",
    "\n",
    "# trajectory_cross_transformer = CrossModalTransformer(\n",
    "#     pose_dim = EMBEDDING_DIM,\n",
    "#     memory_dim= 60,\n",
    "#     embedding_dim=EMBEDDING_DIM,\n",
    "#     num_heads=8,\n",
    "#     num_layers=6,\n",
    "#     use_decoder=True\n",
    "# )\n",
    "\n",
    "trainer = Trainer({\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"train_dataloader\": trainDataLoader,\n",
    "    \"eval_dataloader\": evalDataLoader,\n",
    "    \"clip_model\": clip_model,\n",
    "    \"clip_tokenizer\": clip_tokenizer,\n",
    "    \"pose_transformer\": pose_transformer,\n",
    "    \"text_cross_transformer\": text_cross_transformer,\n",
    "    \"noise_predictor\": noise_predictor,\n",
    "    # \"trajectory_cross_transformer\": trajectory_cross_transformer,\n",
    "    \"pose_features_dim\": POSE_FEATURES_DIM,\n",
    "    \"embedding_dim\": EMBEDDING_DIM,\n",
    "    \"accumulation_steps\": ACCUMULATION_STEPS,\n",
    "}, checkpoint_path=checkpoint_path)\n",
    "# Print for each model\n",
    "print(f\"Pose Transformer Parameters: {count_parameters(trainer.pose_transformer)}\")\n",
    "print(f\"Text-Cross Transformer Parameters: {count_parameters(trainer.text_cross_transformer)}\")\n",
    "print(f\"Noise Predictor Parameters: {count_parameters(trainer.noise_predictor)}\")\n",
    "\n",
    "# Total trainable parameters\n",
    "total_params = sum([\n",
    "    count_parameters(trainer.pose_transformer),\n",
    "    count_parameters(trainer.text_cross_transformer),\n",
    "    count_parameters(trainer.noise_predictor),\n",
    "])\n",
    "print(f\"Total Trainable Parameters: {total_params}\")\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "best_eval_loss = float(\"inf\")\n",
    "best_model_state = None\n",
    "\n",
    "# TODO debugging\n",
    "EPOCHS = 1\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = trainer.train()\n",
    "    train_losses.append(train_loss)\n",
    "    eval_loss = trainer.eval()\n",
    "    eval_losses.append(eval_loss)\n",
    "    print(\"Trian loss: \", train_loss, \"Eval loss: \", eval_loss)\n",
    "    if eval_loss < best_eval_loss:\n",
    "        best_eval_loss = eval_loss\n",
    "        best_model_state = {\n",
    "            \"pose_transformer\": trainer.pose_transformer.state_dict(),\n",
    "            \"text_cross_transformer\": trainer.text_cross_transformer.state_dict(),\n",
    "            \"noise_predictor\": trainer.noise_predictor.state_dict(),\n",
    "        }\n",
    "if best_model_state:\n",
    "    torch.save(best_model_state, checkpoint_path)\n",
    "    print(f\"Best model saved with eval loss {best_eval_loss:.4f} at {checkpoint_path}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, EPOCHS + 1), train_losses, marker='o', linestyle='-', color='b', label=\"Train Loss\")\n",
    "plt.plot(range(1, EPOCHS + 1), eval_losses, marker='s', linestyle='--', color='r', label=\"Eval Loss\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.title(\"Training & Evaluation Loss Curve\")\n",
    "plt.legend()  # Show legend to differentiate train and eval losses\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
