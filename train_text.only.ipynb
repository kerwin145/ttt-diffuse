{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64e77b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kelvin C\\.conda\\envs\\machine_learning\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join as pjoin\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence \n",
    "from tqdm import tqdm\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import PoseTransformer\n",
    "# from PoseTransformer import PoseTransformer, CrossModalTransformer\n",
    "from NoiseScheduler import NoiseScheduler\n",
    "\n",
    "mp.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "ACCUMULATION_STEPS = 4\n",
    "EPOCHS = 7\n",
    "SAVE_PATH = \"model_output\"\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-1\n",
    "PERCENT_TRAIN = 1\n",
    "\n",
    "EMBEDDING_DIM = 512 \n",
    "POSE_FEATURES_DIM = 263\n",
    "LATENT_DIM = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc6cbdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = \"..\\\\HumanML3D\"\n",
    "train_list = open(pjoin(src_dir, \"train.txt\"), \"r\", encoding=\"utf-8\")\n",
    "val_list = open(pjoin(src_dir, \"val.txt\"), \"r\", encoding=\"utf-8\")\n",
    "test_list = open(pjoin(src_dir, \"test.txt\"), \"r\", encoding=\"utf-8\")\n",
    "\n",
    "clip_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_text_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_text_model.eval()\n",
    "for p in clip_text_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a45ece69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualMLPBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(dim, dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x + self.net(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    # ... (Same definition as in AE training script)\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 128), nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    # ... (Same definition as in AE training script)\n",
    "     def __init__(self, latent_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 256), nn.ReLU(),\n",
    "            nn.Linear(256, output_dim)\n",
    "        )\n",
    "     def forward(self, x): return self.net(x)\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "     def __init__(self, input_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_dim, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, input_dim)\n",
    "     def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstruction = self.decoder(latent)\n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa87a2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_embedding(x, num_frequencies=10):\n",
    "    \"\"\"\n",
    "    Applies Fourier feature encoding to the input tensor.\n",
    "    x: (seq_len, 3) - The trajectory data (X, Y, Z)\n",
    "    num_frequencies: The number of frequency bands\n",
    "    \"\"\"\n",
    "    seq_len, dim = x.shape  # dim = 3 (X, Y, Z)\n",
    "    \n",
    "    # Define frequency bands: log-spaced\n",
    "    freqs = torch.logspace(0.0, np.log10(1000.0), num_frequencies)  # Frequencies in range [1, 1000]\n",
    "    \n",
    "    # Compute sin and cos embeddings for each frequency\n",
    "    x_proj = x.unsqueeze(-1) * freqs  # Shape: (seq_len, 3, num_frequencies)\n",
    "    fourier_features = torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)  # (seq_len, 3, 2*num_frequencies)\n",
    "    \n",
    "    return fourier_features.view(seq_len, -1)  # Flatten to (seq_len, 3 * 2 * num_frequencies)\n",
    "\n",
    "class PoseTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "        (data_dir): contains all the new_joint_vecs\n",
    "        (train_list): the train.txt, as an opened file\n",
    "        (max_len): max length of text descriptions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, src_dir, setting, tokenizer, joint_num, use_percentage = 1.0):\n",
    "        self.src_dir = src_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.joint_num = joint_num \n",
    "        self.pose_features_dim = POSE_FEATURES_DIM\n",
    "\n",
    "        if setting not in [\"train\", \"train_temp\", \"val\", \"test\", \"all\"]:\n",
    "            print(\"Invalid setting. Must be train, val, test, or all.\")\n",
    "            raise\n",
    "\n",
    "        with open(pjoin(src_dir, f\"{setting}.txt\"), \"r\") as f:\n",
    "            self.file_list = [line.strip() for line in f.readlines()]\n",
    "\n",
    "        if 0 < use_percentage < 1.0:\n",
    "            random.shuffle(self.file_list)\n",
    "            num_samples = int(len(self.file_list) * use_percentage)\n",
    "            self.file_list = self.file_list[:num_samples]\n",
    "\n",
    "        # get stats to normalize data later on\n",
    "        self.mean, self.std = None, None\n",
    "        stats = np.load(\"pose_stats.npz\")\n",
    "        self.mean = torch.tensor(stats['mean'], dtype=torch.float32).unsqueeze(0) # Shape (1, pose_features_dim)\n",
    "        self.std = torch.tensor(stats['std'], dtype=torch.float32).unsqueeze(0)   # Shape (1, pose_features_dim)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        file = self.file_list[index]\n",
    "\n",
    "        # Load pose data\n",
    "        pose_path = pjoin(self.src_dir, \"new_joint_vecs\", f\"{file}.npy\")\n",
    "        pose_data = np.load(pose_path)  # Shape: (frames, joints, features)\n",
    "        pose_tensor = torch.tensor(pose_data, dtype=torch.float32)\n",
    "        # normalize\n",
    "        pose_tensor_normalized = (pose_tensor - self.mean) / self.std\n",
    "\n",
    "        # Load text description\n",
    "        text_path = pjoin(self.src_dir, \"texts\", f\"{file}.txt\")\n",
    "\n",
    "        # The descriptions have a extra information such as time stamp and part of speech. I will get rid of that for now to keep things simple.\n",
    "        with open(text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text_descriptions = [\n",
    "                re.split('#', line.strip())[0] for line in f.readlines() if line.strip()\n",
    "            ]\n",
    "\n",
    "        # encoded_texts = self.tokenizer(\n",
    "        #     text_descriptions,\n",
    "        #     truncation=True,\n",
    "        #     padding=\"max_length\",\n",
    "        #     max_length=\n",
    "        # _len,\n",
    "        #     return_tensors=\"pt\"\n",
    "        # )\n",
    "\n",
    "        # Since tokenizer returns tensors in a dictionary, we can access them directly\n",
    "        # input_ids = encoded_texts[\"input_ids\"]\n",
    "        # attention_mask = encoded_texts[\"attention_mask\"]\n",
    "\n",
    "        # world_joints = recover_from_ric(torch.from_numpy(pose_data).float(), self.joint_num)\n",
    "        # root_positions = world_joints[:, 0, :] # get trajectory from root position\n",
    "        # fourier_encoded_traj = fourier_embedding(root_positions)\n",
    "\n",
    "        # Return a list of dictionaries, one per description\n",
    "        return [{\n",
    "            \"pose\": pose_tensor_normalized,\n",
    "            \"text\": text,\n",
    "            # \"text\": input_ids[i],\n",
    "            # \"attention_mask\": attention_mask[i],\n",
    "            # \"trajectory\": fourier_encoded_traj\n",
    "        } for text in text_descriptions]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Pads all pose sequences to the maximum length in the batch.\n",
    "    \"\"\"\n",
    "    flattened_batch = [item for sublist in batch for item in sublist]\n",
    "    \n",
    "    # Extract poses and text descriptions from the batch\n",
    "    poses = [item[\"pose\"] for item in flattened_batch]  # List of pose tensors\n",
    "    padded_poses = pad_sequence(poses, batch_first=True, padding_value=0.0)  # Shape: (batch_size, max_len, ...)\n",
    "    pose_key_padding = (padded_poses.sum(-1) == 0)  \n",
    "\n",
    "    # trajectories = [item[\"trajectory\"] for item in flattened_batch]  # Fourier-encoded trajectories (varied lengths)\n",
    "    # Pad trajectory sequences (pad with zeros to max length in the batch)\n",
    "    # padded_trajectories = pad_sequence(trajectories, batch_first=True, padding_value=0.0)  # Shape: (batch_size, max_traj_len, fourier_dim)\n",
    "\n",
    "    texts = [item[\"text\"] for item in flattened_batch]  # List of tokenized text tensors\n",
    "    tokenized = clip_tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,        # pad to longest in this *batch*\n",
    "        truncation=True,\n",
    "        max_length=None      # optional: drop any hard max\n",
    "    )\n",
    "\n",
    "    input_ids      = tokenized.input_ids\n",
    "    attention_mask = tokenized.attention_mask\n",
    "\n",
    "    return {\n",
    "        \"pose\": padded_poses,\n",
    "        \"pose_mask\": pose_key_padding,\n",
    "        \"text\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        # \"trajectory\": padded_trajectories,\n",
    "        # trajectory_mask not needed as it shares the same dimensions as pose_mask\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c460be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, options, checkpoint_path = None):\n",
    "        self.device = options['device']\n",
    "        self.train_dataloader = options['train_dataloader']\n",
    "        self.eval_dataloader = options['eval_dataloader']\n",
    "        self.clip_text_model = options['clip_text_model']\n",
    "        self.clip_tokenizer = options['clip_tokenizer']\n",
    "        self.pose_transformer = options['pose_transformer']\n",
    "        self.text_cross_transformer = options['text_cross_transformer']\n",
    "        # self.trajectory_cross_transformer = options['trajectory_cross_transformer'].to(self.device)\n",
    "        self.cfg_dropout_prob = options['cfg_dropout_prob']\n",
    "        \n",
    "        self.autoencoder = options['autoencoder']\n",
    "        self.noise_predictor = options['noise_predictor']\n",
    "        \n",
    "        self.accumulation_steps = options['accumulation_steps']\n",
    "        self.pose_encoder = self.autoencoder.encoder\n",
    "\n",
    "        self.noise_scheduler = NoiseScheduler(timesteps=1000)\n",
    "        # send tensors to device\n",
    "        self.noise_scheduler.betas = self.noise_scheduler.betas.to(self.device)\n",
    "        self.noise_scheduler.alphas = self.noise_scheduler.alphas.to(self.device)\n",
    "        self.noise_scheduler.alphas_cumprod = self.noise_scheduler.alphas_cumprod.to(self.device)\n",
    "        self.noise_scheduler.sqrt_alphas_cumprod = self.noise_scheduler.sqrt_alphas_cumprod.to(self.device)\n",
    "        self.noise_scheduler.sqrt_one_minus_alphas_cumprod = self.noise_scheduler.sqrt_one_minus_alphas_cumprod.to(self.device)\n",
    "        self.snr_gamma = 5.0\n",
    "\n",
    "        # clip is left out of the optimizer, as we won't be tuning CLIP model\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            list(self.pose_transformer.parameters()) +\n",
    "            list(self.text_cross_transformer.parameters()) +\n",
    "            # list(self.trajectory_cross_transformer.parameters()) +\n",
    "            list(self.noise_predictor.parameters()),\n",
    "            lr=LEARNING_RATE,\n",
    "            weight_decay=WEIGHT_DECAY\n",
    "        )\n",
    "\n",
    "        num_batches = len(self.train_dataloader) // ACCUMULATION_STEPS\n",
    "        # Add 1 if there are leftover batches that will trigger an optimizer step\n",
    "        if len(self.train_dataloader) % ACCUMULATION_STEPS != 0:\n",
    "            num_batches += 1\n",
    "        training_steps = num_batches * EPOCHS\n",
    "        warmup_steps = int(training_steps * 0.15)\n",
    "        self.lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=warmup_steps,  # Gradually increase LR\n",
    "            num_training_steps=training_steps  # Cosine decay over training\n",
    "        )\n",
    "\n",
    "        self.pose_transformer.train()\n",
    "        self.text_cross_transformer.train()\n",
    "        # self.trajectory_cross_transformer.train()\n",
    "        self.noise_predictor.train()\n",
    "\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "\n",
    "    def save_model(self):\n",
    "        \"\"\"Save only the model weights without optimizer state.\"\"\"\n",
    "        state = {\n",
    "            \"pose_transformer\": self.pose_transformer.state_dict(),\n",
    "            \"text_cross_transformer\": self.text_cross_transformer.state_dict(),\n",
    "            \"noise_predictor\": self.noise_predictor.state_dict(),\n",
    "        }\n",
    "        torch.save(state, self.checkpoint_path)\n",
    "        print(f\"Model weights saved to {self.checkpoint_path}\")\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Load model weights if a checkpoint exists, ignoring optimizer.\"\"\"\n",
    "        if os.path.exists(self.checkpoint_path):\n",
    "            checkpoint = torch.load(self.checkpoint_path, map_location=self.device)\n",
    "            self.pose_transformer.load_state_dict(checkpoint[\"pose_transformer\"])\n",
    "            self.text_cross_transformer.load_state_dict(checkpoint[\"text_cross_transformer\"])\n",
    "            self.noise_predictor.load_state_dict(checkpoint[\"noise_predictor\"])\n",
    "            print(f\"Model weights loaded from {self.checkpoint_path}\")\n",
    "\n",
    "    # def get_lr_lambda(self):\n",
    "    #     return lambda step: min((step + 1) / self.warmup_steps, 1.0)\n",
    "    \n",
    "    def _process_batch(self, batch, compute_loss = True, use_weighted_loss = True):\n",
    "        poses = batch[\"pose\"].to(self.device)\n",
    "        pose_mask = batch[\"pose_mask\"].to(self.device)\n",
    "        texts = batch[\"text\"].to(self.device)\n",
    "        text_mask = batch[\"attention_mask\"].to(self.device)\n",
    "        # trajectory = batch[\"trajectory\"].to(self.device)\n",
    "\n",
    "        # Denoiser setup\n",
    "        batch_size = poses.shape[0]\n",
    "        seq_len = poses.shape[1]\n",
    "\n",
    "        poses_flat = poses.view(-1, POSE_FEATURES_DIM)\n",
    "        with torch.no_grad(): # Ensure no gradients through AE\n",
    "             latent_poses_flat = self.pose_encoder(poses_flat)\n",
    "        latent_poses = latent_poses_flat.view(batch_size, seq_len, LATENT_DIM)\n",
    "\n",
    "        noise = torch.randn_like(latent_poses)\n",
    "        timesteps = self.noise_scheduler.sample_timesteps(batch_size, device=self.device)\n",
    "        noisy_latent_poses = self.noise_scheduler.add_noise(latent_poses, noise, timesteps)\n",
    "\n",
    "        # Get token-wise text embeddings from CLIPTextModel\n",
    "        with torch.no_grad(): # Ensure no gradients computed for CLIP\n",
    "            txt_out = self.clip_text_model(\n",
    "                input_ids=texts,\n",
    "                attention_mask=text_mask,\n",
    "                return_dict=True\n",
    "            )\n",
    "            text_embeddings = txt_out.last_hidden_state\n",
    "\n",
    "        if self.cfg_dropout_prob > 0.0 and self.training:\n",
    "            # Randomly drop condition with probability `cfg_dropout_prob`\n",
    "            drop_mask = torch.rand(batch_size, device=text_embeddings.device) < self.cfg_dropout_prob\n",
    "            drop_mask = drop_mask.view(batch_size, 1, 1)  # (B, 1, 1) for broadcasting\n",
    "\n",
    "            # Replace dropped samples with zeros\n",
    "            text_embeddings = text_embeddings * (~drop_mask)  # Drop entire text embedding if mask is True\n",
    "\n",
    "        pose_embeddings = self.pose_transformer(\n",
    "            noisy_latent_poses,\n",
    "            timesteps=timesteps,\n",
    "            pose_mask=pose_mask\n",
    "        )\n",
    "\n",
    "        # trajectory_conditioned_embeddings = self.trajectory_cross_transformer(\n",
    "        #     pose_embeddings,\n",
    "        #     trajectory,\n",
    "        #     pose_mask=pose_mask,\n",
    "        #     memory_mask=pose_mask\n",
    "        # )\n",
    "\n",
    "        # Cross-attention with text embeddings\n",
    "        text_pad_mask = (text_mask == 0)\n",
    "        text_conditioned_embeddings = self.text_cross_transformer(\n",
    "            pose_embeddings,\n",
    "            text_embeddings,\n",
    "            pose_mask=pose_mask,\n",
    "            memory_mask=text_pad_mask\n",
    "        )\n",
    "        # Predict noise\n",
    "        predicted_noise = self.noise_predictor(text_conditioned_embeddings)\n",
    "\n",
    "        if compute_loss:\n",
    "            mask = ~pose_mask  # Padding mask (B, T)\n",
    "            mask = mask.unsqueeze(-1)  # (B, T, 1)\n",
    "            diff = (predicted_noise - noise) ** 2\n",
    "            masked_diff = diff * mask\n",
    "\n",
    "            if use_weighted_loss:\n",
    "                alphas_cumprod_t = self.noise_scheduler.alphas_cumprod[timesteps]\n",
    "                epsilon = 1e-8\n",
    "                snr = alphas_cumprod_t / (1.0 - alphas_cumprod_t + epsilon) # Shape: (batch_size,)\n",
    "                # Create a tensor of gamma values with the same shape as snr\n",
    "                gamma_tensor = torch.full_like(snr, self.snr_gamma)\n",
    "                # Calculate the minimum between SNR and gamma for each element\n",
    "                snr_clipped_weight = torch.minimum(snr, gamma_tensor) # Shape: (batch_size,)\n",
    "\n",
    "                weight = snr_clipped_weight.view(-1, 1, 1)\n",
    "                weighted_masked_diff = weight * masked_diff\n",
    "                loss = torch.sum(weighted_masked_diff) / mask.sum().clamp(min=1.0) # mask.sum() counts number of valid elements (B*T*F)\n",
    "            else:\n",
    "                loss = torch.sum(masked_diff) / mask.sum().clamp(min=1.0)\n",
    "\n",
    "            return predicted_noise, noise, timesteps, loss\n",
    "        return predicted_noise, noise, timesteps\n",
    "\n",
    "    def train(self):\n",
    "        self.pose_transformer.train()\n",
    "        self.text_cross_transformer.train()\n",
    "        # self.trajectory_cross_transformer.train()\n",
    "        self.noise_predictor.train()\n",
    "        self.training = True\n",
    "\n",
    "        dataloader = self.train_dataloader\n",
    "        total_loss = 0\n",
    "        num_batches = len(dataloader)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        for i, batch in enumerate(tqdm(dataloader, leave=True)):\n",
    "            _, _, _, raw_loss = self._process_batch(batch, use_weighted_loss=True)\n",
    "            loss = raw_loss / self.accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            if (i + 1) % self.accumulation_steps == 0 or (i + 1) == num_batches: #accumulate gradients, and then update.\n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(self.pose_transformer.parameters(), 1.0)\n",
    "                torch.nn.utils.clip_grad_norm_(self.text_cross_transformer.parameters(), 1.0)\n",
    "                torch.nn.utils.clip_grad_norm_(self.noise_predictor.parameters(), 1.0)\n",
    "\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                self.lr_scheduler.step()\n",
    "\n",
    "            total_loss += raw_loss.item()\n",
    "\n",
    "        return total_loss / num_batches\n",
    "\n",
    "    def eval(self):\n",
    "        self.pose_transformer.eval()\n",
    "        self.text_cross_transformer.eval()\n",
    "        # self.trajectory_cross_transformer.eval()\n",
    "        self.noise_predictor.eval()\n",
    "        self.training = False\n",
    "        \n",
    "        dataloader = self.eval_dataloader\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            for batch in tqdm(dataloader, leave=True):\n",
    "                _, _, _, loss = self._process_batch(batch, use_weighted_loss=True)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        return total_loss / len(dataloader)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def init_weights_transformer(module):\n",
    "    \"\"\"\n",
    "    Initialize weights using Transformer-style N(0, 0.02) for Linear & Embedding.\n",
    "    Initialize LayerNorm weights to 1 and biases to 0.\n",
    "    \"\"\"\n",
    "    std_dev = 0.02 # Standard deviation for normal initialization\n",
    "    if isinstance(module, nn.Linear):\n",
    "        # Normal initialization for weights\n",
    "        nn.init.normal_(module.weight, mean=0.0, std=std_dev)\n",
    "        # Zero initialization for biases if they exist\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "        # Normal initialization for embedding weights\n",
    "        nn.init.normal_(module.weight, mean=0.0, std=std_dev)\n",
    "        # Zero out padding embedding if it exists\n",
    "        if module.padding_idx is not None:\n",
    "            with torch.no_grad(): # Ensure this operation isn't tracked by autograd\n",
    "                module.weight[module.padding_idx].fill_(0)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        # Initialize weights (gamma) to 1\n",
    "        nn.init.ones_(module.weight)\n",
    "        # Initialize biases (beta) to 0\n",
    "        nn.init.zeros_(module.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "419052c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Train dataset\n",
      "Loading Eval Dataset\n",
      "Dataset loading done\n",
      "Autoencoder weights loaded successfully.\n",
      "Pose Transformer Parameters: 28916736\n",
      "Text-Cross Transformer Parameters: 38361600\n",
      "Noise Predictor Parameters: 820800\n",
      "Total Trainable Parameters: 68099136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2923 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Trainer' object has no attribute 'training'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 114\u001b[0m\n\u001b[0;32m    111\u001b[0m best_model_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m--> 114\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m    116\u001b[0m     eval_loss \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[1;32mIn[5], line 177\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(dataloader, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)):\n\u001b[1;32m--> 177\u001b[0m     _, _, _, raw_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_weighted_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m     loss \u001b[38;5;241m=\u001b[39m raw_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulation_steps\n\u001b[0;32m    179\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[1;32mIn[5], line 108\u001b[0m, in \u001b[0;36mTrainer._process_batch\u001b[1;34m(self, batch, compute_loss, use_weighted_loss)\u001b[0m\n\u001b[0;32m    101\u001b[0m     txt_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_text_model(\n\u001b[0;32m    102\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[0;32m    103\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mtext_mask,\n\u001b[0;32m    104\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    105\u001b[0m     )\n\u001b[0;32m    106\u001b[0m     text_embeddings \u001b[38;5;241m=\u001b[39m txt_out\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg_dropout_prob \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m:\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;66;03m# Randomly drop condition with probability `cfg_dropout_prob`\u001b[39;00m\n\u001b[0;32m    110\u001b[0m     drop_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(batch_size, device\u001b[38;5;241m=\u001b[39mtext_embeddings\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg_dropout_prob\n\u001b[0;32m    111\u001b[0m     drop_mask \u001b[38;5;241m=\u001b[39m drop_mask\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B, 1, 1) for broadcasting\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Trainer' object has no attribute 'training'"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)  # Python random module\n",
    "np.random.seed(SEED)  # NumPy random\n",
    "torch.manual_seed(SEED)  # PyTorch random\n",
    "\n",
    "# importlib.reload(PoseTransformer)\n",
    "\n",
    "POSE_TRANSFORMER_LAYERS = 8\n",
    "CROSS_TRANSFORMER_LAYERS = 8\n",
    "CFG_DROPOUT_RATE = 0.1\n",
    "\n",
    "# 5.1.0 uses SNR weighting\n",
    "# 5.1.1 changes cross transformer layers from 6 to 4\n",
    "# 6 introducd AE\n",
    "# 7 introduced CFG\n",
    "\n",
    "checkpoint_path = f\"model_saves/v7.0.0_TT_datapercent{PERCENT_TRAIN}_lr{LEARNING_RATE}_WD{WEIGHT_DECAY}_P{POSE_TRANSFORMER_LAYERS}C{CROSS_TRANSFORMER_LAYERS}_CFG{CFG_DROPOUT_RATE}\"\n",
    "\n",
    "print(\"Loading Train dataset\")\n",
    "trainDataset = PoseTextDataset(src_dir=src_dir,setting=\"train\",tokenizer=clip_tokenizer, joint_num=22, use_percentage=PERCENT_TRAIN)\n",
    "trainDataLoader = DataLoader(trainDataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=0,collate_fn=collate_fn)\n",
    "# persistent_workers=True,\n",
    "# multiprocessing_context=\"spawn\"\n",
    "print(\"Loading Eval Dataset\")\n",
    "evalDataset = PoseTextDataset(src_dir=src_dir,setting=\"val\",tokenizer=clip_tokenizer, joint_num=22, use_percentage=1)\n",
    "evalDataLoader = DataLoader(evalDataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=0,collate_fn=collate_fn)\n",
    "print(\"Dataset loading done\")\n",
    "pose_transformer = PoseTransformer.PoseTransformer(\n",
    "    pose_dim=LATENT_DIM,\n",
    "    embedding_dim=EMBEDDING_DIM,  # Match CLIP's embedding dimension\n",
    "    num_heads=8,  # Number of attention heads\n",
    "    num_layers=POSE_TRANSFORMER_LAYERS,  # Number of transformer layers\n",
    "    dropout=CFG_DROPOUT_RATE,  # Dropout probability\n",
    "    use_decoder=False\n",
    ").to(device)\n",
    "\n",
    "text_cross_transformer = PoseTransformer.CrossModalTransformer(\n",
    "    pose_dim = EMBEDDING_DIM,\n",
    "    memory_dim= EMBEDDING_DIM,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    num_heads=8,\n",
    "    num_layers=CROSS_TRANSFORMER_LAYERS,\n",
    "    use_decoder=True\n",
    ").to(device)\n",
    "\n",
    "noise_predictor = nn.Sequential(\n",
    "    ResidualMLPBlock(EMBEDDING_DIM),\n",
    "    nn.Linear(EMBEDDING_DIM, EMBEDDING_DIM),\n",
    "    nn.SiLU(),\n",
    "    nn.Linear(EMBEDDING_DIM, LATENT_DIM),\n",
    ").to(device)\n",
    "\n",
    "autoencoder = Autoencoder(POSE_FEATURES_DIM, LATENT_DIM).to(device)\n",
    "ae_checkpoint = torch.load(\"model_saves\\\\autoencoder\\\\pose_ae_best.pth\", map_location=device, weights_only=True)\n",
    "# Verify dimensions match\n",
    "if ae_checkpoint['input_dim'] != POSE_FEATURES_DIM:\n",
    "        raise ValueError(f\"AE Checkpoint input_dim ({ae_checkpoint['input_dim']}) doesn't match expected ({POSE_FEATURES_DIM})\")\n",
    "if ae_checkpoint['latent_dim'] != LATENT_DIM:\n",
    "        raise ValueError(f\"AE Checkpoint latent_dim ({ae_checkpoint['latent_dim']}) doesn't match expected ({LATENT_DIM})\")\n",
    "\n",
    "autoencoder.load_state_dict(ae_checkpoint['model_state_dict'])\n",
    "print(\"Autoencoder weights loaded successfully.\")\n",
    "autoencoder.eval()\n",
    "for param in autoencoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "pose_transformer.apply(init_weights_transformer)\n",
    "text_cross_transformer.apply(init_weights_transformer)\n",
    "noise_predictor.apply(init_weights_transformer) # Apply standard init first\n",
    "\n",
    "# trajectory_cross_transformer = CrossModalTransformer(\n",
    "#     pose_dim = EMBEDDING_DIM,\n",
    "#     memory_dim= 60,\n",
    "#     embedding_dim=EMBEDDING_DIM,\n",
    "#     num_heads=8,\n",
    "#     num_layers=6,\n",
    "#     use_decoder=True\n",
    "# )\n",
    "\n",
    "trainer = Trainer({\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"train_dataloader\": trainDataLoader,\n",
    "    \"eval_dataloader\": evalDataLoader,\n",
    "    \"clip_text_model\": clip_text_model,\n",
    "    \"clip_tokenizer\": clip_tokenizer,\n",
    "    \"pose_transformer\": pose_transformer,\n",
    "    \"text_cross_transformer\": text_cross_transformer,\n",
    "    # \"trajectory_cross_transformer\": trajectory_cross_transformer,\n",
    "    \"cfg_dropout_prob\": 0.1,\n",
    "    \"autoencoder\": autoencoder,\n",
    "    \"noise_predictor\": noise_predictor,\n",
    "    \"pose_features_dim\": POSE_FEATURES_DIM,\n",
    "    \"embedding_dim\": EMBEDDING_DIM,\n",
    "    \"accumulation_steps\": ACCUMULATION_STEPS,\n",
    "}, checkpoint_path=checkpoint_path)\n",
    "# Print for each model\n",
    "print(f\"Pose Transformer Parameters: {count_parameters(trainer.pose_transformer)}\")\n",
    "print(f\"Text-Cross Transformer Parameters: {count_parameters(trainer.text_cross_transformer)}\")\n",
    "print(f\"Noise Predictor Parameters: {count_parameters(trainer.noise_predictor)}\")\n",
    "\n",
    "# Total trainable parameters\n",
    "total_params = sum([\n",
    "    count_parameters(trainer.pose_transformer),\n",
    "    count_parameters(trainer.text_cross_transformer),\n",
    "    count_parameters(trainer.noise_predictor),\n",
    "])\n",
    "print(f\"Total Trainable Parameters: {total_params}\")\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "best_eval_loss = float(\"inf\")\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = trainer.train()\n",
    "    train_losses.append(train_loss)\n",
    "    eval_loss = trainer.eval()\n",
    "    eval_losses.append(eval_loss)\n",
    "    print(\"Trian loss: \", train_loss, \"Eval loss: \", eval_loss)\n",
    "    if eval_loss < best_eval_loss:\n",
    "        best_eval_loss = eval_loss\n",
    "        best_model_state = {\n",
    "            \"pose_transformer\": trainer.pose_transformer.state_dict(),\n",
    "            \"text_cross_transformer\": trainer.text_cross_transformer.state_dict(),\n",
    "            \"noise_predictor\": trainer.noise_predictor.state_dict(),\n",
    "        }\n",
    "\n",
    "if best_model_state:\n",
    "    torch.save(best_model_state, checkpoint_path)\n",
    "    print(f\"Best model saved with eval loss {best_eval_loss:.4f} at {checkpoint_path}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, EPOCHS + 1), train_losses, marker='o', linestyle='-', color='b', label=\"Train Loss\")\n",
    "plt.plot(range(1, EPOCHS + 1), eval_losses, marker='s', linestyle='--', color='r', label=\"Eval Loss\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.title(\"Training & Evaluation Loss Curve\")\n",
    "plt.legend()  # Show legend to differentiate train and eval losses\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
