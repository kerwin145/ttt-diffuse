{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading models...\n",
      "Model weights loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "from PoseTransformer import PoseTransformer, CrossModalTransformer\n",
    "from NoiseScheduler import NoiseScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from utils.motion_process import recover_from_ric\n",
    "\n",
    "class ResidualMLPBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(dim, dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x + self.net(x)\n",
    "    \n",
    "MODEL_PATH = \"model_saves/v4.1.1_TT_1_percentdata_lr0.0001_wd0.0001\"\n",
    "EMBEDDING_DIM = 512\n",
    "POSE_FEATURES_DIM = 263\n",
    "CLIP_MAX_LENGTH = 77\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load trained models\n",
    "print(\"Loading models...\")\n",
    "pose_transformer = PoseTransformer(\n",
    "    pose_dim=POSE_FEATURES_DIM,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    num_heads=8,\n",
    "    num_layers=8,\n",
    "    dropout=0.1,\n",
    "    use_decoder=False\n",
    ").to(device)\n",
    "\n",
    "text_cross_transformer = CrossModalTransformer(\n",
    "    pose_dim=EMBEDDING_DIM,\n",
    "    memory_dim=EMBEDDING_DIM,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    num_heads=8,\n",
    "    num_layers=8,\n",
    "    use_decoder=True\n",
    ").to(device)\n",
    "\n",
    "noise_predictor = nn.Sequential(\n",
    "    ResidualMLPBlock(EMBEDDING_DIM),\n",
    "    nn.Linear(EMBEDDING_DIM, EMBEDDING_DIM),\n",
    "    nn.SiLU(),\n",
    "    nn.Linear(EMBEDDING_DIM, POSE_FEATURES_DIM),\n",
    ")\n",
    "\n",
    "\n",
    "# CLIP Model and Tokenizer\n",
    "clip_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_text_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    raise FileNotFoundError(f\"Model checkpoint not found at: {MODEL_PATH}\")\n",
    "# weights only to true to stop receiving the warning\n",
    "checkpoint = torch.load(MODEL_PATH, map_location=device, weights_only=True)\n",
    "\n",
    "pose_transformer.load_state_dict(checkpoint[\"pose_transformer\"])\n",
    "text_cross_transformer.load_state_dict(checkpoint[\"text_cross_transformer\"])\n",
    "noise_predictor.load_state_dict(checkpoint[\"noise_predictor\"])\n",
    "print(\"Model weights loaded successfully.\")\n",
    "\n",
    "pose_transformer.eval()\n",
    "text_cross_transformer.eval()\n",
    "noise_predictor.eval()\n",
    "clip_text_model.eval()\n",
    "\n",
    "noise_scheduler = NoiseScheduler(timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def infer(text_input, seq_length=60, batch_size=1):\n",
    "\n",
    "    # Tokenize text input and get text embeddings\n",
    "    tokenized = clip_tokenizer(\n",
    "        text_input,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,        # pad to longest in this *batch*\n",
    "        truncation=True,\n",
    "        max_length=None      # optional: drop any hard max\n",
    "    )\n",
    "    input_ids      = tokenized.input_ids\n",
    "    attention_mask = tokenized.attention_mask\n",
    "\n",
    "    txt_out = clip_text_model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        return_dict=True\n",
    "    )\n",
    "    text_embeddings = txt_out.last_hidden_state\n",
    "\n",
    "    # Initialize random noise as the starting pose\n",
    "    pose_shape = (batch_size, seq_length, POSE_FEATURES_DIM)\n",
    "    current_pose = torch.randn(pose_shape, device=device)\n",
    "    pose_mask = torch.zeros((batch_size, seq_length), device=device, dtype=torch.float32) # <-- Create mask\n",
    "\n",
    "    print(\"Starting denoising loop...\")\n",
    "    print(current_pose.shape, pose_mask.shape)\n",
    "\n",
    "    # Perform reverse diffusion (denoising)\n",
    "    for t in reversed(range(noise_scheduler.timesteps)):\n",
    "        timesteps = torch.full((batch_size,), t, device=device, dtype=torch.long)\n",
    "\n",
    "        # Get pose embeddings from transformer\n",
    "        pose_embeddings = pose_transformer(\n",
    "            current_pose,\n",
    "            pose_mask=pose_mask,\n",
    "            timesteps=timesteps\n",
    "        )\n",
    "\n",
    "        # Cross-attention with text embeddings\n",
    "        text_conditioned_embeddings = text_cross_transformer(\n",
    "            pose_embeddings,\n",
    "            text_embeddings,\n",
    "            pose_mask, \n",
    "            memory_mask=None\n",
    "        )\n",
    "\n",
    "        # Predict noise\n",
    "        predicted_noise = noise_predictor(text_conditioned_embeddings)\n",
    "\n",
    "        beta_t = noise_scheduler.betas[t].to(device)\n",
    "        alpha_t = noise_scheduler.alphas[t].to(device)\n",
    "        sqrt_one_minus_alpha_cumprod_t = noise_scheduler.sqrt_one_minus_alphas_cumprod[t].to(device)\n",
    "        sqrt_alpha_t = torch.sqrt(alpha_t) \n",
    "\n",
    "        # Calculate the term multiplying the predicted noise\n",
    "        noise_coeff = beta_t / sqrt_one_minus_alpha_cumprod_t\n",
    "\n",
    "        # Calculate the mean of x_{t-1}\n",
    "        mean_x_t_minus_1 = (1.0 / sqrt_alpha_t) * (current_pose - noise_coeff * predicted_noise)\n",
    "\n",
    "        # Add noise (variance term) - except for the last step (t=0)\n",
    "        if t > 0:\n",
    "            variance = beta_t # Use beta_t for variance (sigma_t = sqrt(beta_t))\n",
    "            std_dev = torch.sqrt(variance)\n",
    "            noise = torch.randn_like(current_pose)\n",
    "            current_pose = mean_x_t_minus_1 + std_dev * noise # Update current_pose to x_{t-1}\n",
    "        else:\n",
    "            current_pose = mean_x_t_minus_1\n",
    "\n",
    "    return current_pose.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting denoising loop...\n",
      "torch.Size([1, 40, 263]) torch.Size([1, 40])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "to_padded_tensor: at least one constituent tensor should have non-zero numel",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m      2\u001b[0m text_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA person walking in a straight line.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m generated_poses \u001b[38;5;241m=\u001b[39m \u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m pose_single \u001b[38;5;241m=\u001b[39m generated_poses[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Kelvin C\\.conda\\envs\\machine_learning\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[12], line 35\u001b[0m, in \u001b[0;36minfer\u001b[1;34m(text_input, seq_length, batch_size)\u001b[0m\n\u001b[0;32m     32\u001b[0m timesteps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((batch_size,), t, device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Get pose embeddings from transformer\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m pose_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mpose_transformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcurrent_pose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpose_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpose_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimesteps\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Cross-attention with text embeddings\u001b[39;00m\n\u001b[0;32m     42\u001b[0m text_conditioned_embeddings \u001b[38;5;241m=\u001b[39m text_cross_transformer(\n\u001b[0;32m     43\u001b[0m     pose_embeddings,\n\u001b[0;32m     44\u001b[0m     text_embeddings,\n\u001b[0;32m     45\u001b[0m     pose_mask, \n\u001b[0;32m     46\u001b[0m     memory_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     47\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Kelvin C\\.conda\\envs\\machine_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Kelvin C\\.conda\\envs\\machine_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Kelvin C\\Desktop\\DEVELOPERS\\TT-difuse\\t2m-diffuse\\PoseTransformer.py:76\u001b[0m, in \u001b[0;36mPoseTransformer.forward\u001b[1;34m(self, poses, timesteps, pose_mask)\u001b[0m\n\u001b[0;32m     73\u001b[0m t_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestep_embedding(timesteps)  \u001b[38;5;66;03m# (batch_size, embedding_dim)\u001b[39;00m\n\u001b[0;32m     74\u001b[0m pose_embeddings \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m t_emb\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Broadcast to (batch_size, seq_len, embedding_di\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m encoded_poses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpose_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpose_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoded_poses\n",
      "File \u001b[1;32mc:\\Users\\Kelvin C\\.conda\\envs\\machine_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Kelvin C\\.conda\\envs\\machine_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Kelvin C\\.conda\\envs\\machine_learning\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:519\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    511\u001b[0m     output \u001b[38;5;241m=\u001b[39m mod(\n\u001b[0;32m    512\u001b[0m         output,\n\u001b[0;32m    513\u001b[0m         src_mask\u001b[38;5;241m=\u001b[39mmask,\n\u001b[0;32m    514\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[0;32m    515\u001b[0m         src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask_for_layers,\n\u001b[0;32m    516\u001b[0m     )\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m--> 519\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_padded_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    522\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(output)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: to_padded_tensor: at least one constituent tensor should have non-zero numel"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text_prompt = \"A person walking in a straight line.\"\n",
    "generated_poses = infer(text_prompt, seq_length=40, batch_size=1)\n",
    "pose_single = generated_poses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.tensor(np.load(\"pose_stats.npz\")['mean'], dtype=torch.float32).unsqueeze(0)\n",
    "std = torch.tensor(np.load(\"pose_stats.npz\")['std'], dtype=torch.float32).unsqueeze(0)\n",
    "pose_single_denorm = pose_single * std + mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.animation import FuncAnimation, FFMpegFileWriter\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "\n",
    "def plot_3d_motion(save_path, kinematic_tree, joints, title, figsize=(10, 10), fps=120, radius=4):\n",
    "    matplotlib.use('Agg')\n",
    "\n",
    "    title_sp = title.split(' ')\n",
    "    if len(title_sp) > 10:\n",
    "        title = '\\n'.join([' '.join(title_sp[:10]), ' '.join(title_sp[10:])])\n",
    "    def init():\n",
    "        ax.set_xlim3d([-radius / 2, radius / 2])\n",
    "        ax.set_ylim3d([0, radius])\n",
    "        ax.set_zlim3d([0, radius])\n",
    "        # print(title)\n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        ax.grid(b=False)\n",
    "\n",
    "    def plot_xzPlane(minx, maxx, miny, minz, maxz):\n",
    "        ## Plot a plane XZ\n",
    "        verts = [\n",
    "            [minx, miny, minz],\n",
    "            [minx, miny, maxz],\n",
    "            [maxx, miny, maxz],\n",
    "            [maxx, miny, minz]\n",
    "        ]\n",
    "        xz_plane = Poly3DCollection([verts])\n",
    "        xz_plane.set_facecolor((0.5, 0.5, 0.5, 0.5))\n",
    "        ax.add_collection3d(xz_plane)\n",
    "\n",
    "    #         return ax\n",
    "\n",
    "    # (seq_len, joints_num, 3)\n",
    "    data = joints.copy().reshape(len(joints), -1, 3)\n",
    "    # fig = plt.figure(figsize=figsize)\n",
    "    # ax = p3.Axes3D(fig)\n",
    "    fig, ax = plt.subplots(subplot_kw={'projection': '3d'}, figsize=figsize)  # Use subplots to create a 3D axis\n",
    "\n",
    "    init()\n",
    "    MINS = data.min(axis=0).min(axis=0)\n",
    "    MAXS = data.max(axis=0).max(axis=0)\n",
    "    colors = ['red', 'blue', 'black', 'red', 'blue',  \n",
    "              'darkblue', 'darkblue', 'darkblue', 'darkblue', 'darkblue',\n",
    "             'darkred', 'darkred','darkred','darkred','darkred']\n",
    "    frame_number = data.shape[0]\n",
    "    #     print(data.shape)\n",
    "\n",
    "    height_offset = MINS[1]\n",
    "    data[:, :, 1] -= height_offset\n",
    "    trajec = data[:, 0, [0, 2]]\n",
    "    \n",
    "    data[..., 0] -= data[:, 0:1, 0]\n",
    "    data[..., 2] -= data[:, 0:1, 2]\n",
    "\n",
    "    #     print(trajec.shape)\n",
    "\n",
    "    def update(index):\n",
    "        #         print(index)\n",
    "        # ax.lines = []\n",
    "        # ax.collections = []\n",
    "        ax.cla()\n",
    "        ax.view_init(elev=120, azim=-90)\n",
    "        ax.dist = 7.5\n",
    "        #         ax =\n",
    "        plot_xzPlane(MINS[0]-trajec[index, 0], MAXS[0]-trajec[index, 0], 0, MINS[2]-trajec[index, 1], MAXS[2]-trajec[index, 1])\n",
    "#         ax.scatter(data[index, :22, 0], data[index, :22, 1], data[index, :22, 2], color='black', s=3)\n",
    "        \n",
    "        if index > 1:\n",
    "            ax.plot3D(trajec[:index, 0]-trajec[index, 0], np.zeros_like(trajec[:index, 0]), trajec[:index, 1]-trajec[index, 1], linewidth=1.0,\n",
    "                      color='blue')\n",
    "        #             ax = plot_xzPlane(ax, MINS[0], MAXS[0], 0, MINS[2], MAXS[2])\n",
    "        \n",
    "        \n",
    "        for i, (chain, color) in enumerate(zip(kinematic_tree, colors)):\n",
    "#             print(color)\n",
    "            if i < 5:\n",
    "                linewidth = 4.0\n",
    "            else:\n",
    "                linewidth = 2.0\n",
    "            ax.plot3D(data[index, chain, 0], data[index, chain, 1], data[index, chain, 2], linewidth=linewidth, color=color)\n",
    "        #         print(trajec[:index, 0].shape)\n",
    "\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_zticklabels([])\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=frame_number, interval=1000/fps, repeat=False)\n",
    "\n",
    "    ani.save(save_path, fps=fps)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from utils.utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "joints_num = 22\n",
    "kinematic_chain = [[0, 2, 5, 8, 11], [0, 1, 4, 7, 10], [0, 3, 6, 9, 12, 15], [9, 14, 17, 19, 21], [9, 13, 16, 18, 20]]\n",
    "# print(example_data_ml3d[0:5])\n",
    "\n",
    "joint = recover_from_ric(pose_single.float(), joints_num).numpy()\n",
    "joint = motion_temporal_filter(joint, sigma=1)\n",
    "\n",
    "trajectory = joint[:, 0, :]\n",
    "x = trajectory[:, 0]\n",
    "z = trajectory[:, 2]\n",
    "n = len(x)\n",
    "plot_3d_motion(\"output/test_ani5.mp4\", kinematic_chain, joint, title=\"Testing!\", fps=10)\n",
    "# colors = plt.cm.viridis(np.linspace(0, 1, n))\n",
    "# plt.scatter(x, z, c=colors, s=5)\n",
    "# plt.plot(x, z, linewidth=1)  # Plot X-Z movement\n",
    "# plt.xlabel('X')\n",
    "# plt.ylabel('Z')\n",
    "# plt.title('Root Joint Trajectory')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
