{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kelvin C\\.conda\\envs\\machine_learning\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading models...\n",
      "1000\n",
      "Model weights loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from PoseTransformer import PoseTransformer, CrossModalTransformer\n",
    "from NoiseScheduler import NoiseScheduler\n",
    "from transformers import CLIPModel, CLIPTokenizer\n",
    "from utils.motion_process import recover_from_ric\n",
    "\n",
    "MODEL_PATH = \"model_saves/TT_0.75_percentdata_lr0.0001_wd0.01\"\n",
    "EMBEDDING_DIM = 512\n",
    "POSE_FEATURES_DIM = 263\n",
    "CLIP_MAX_LENGTH = 77\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load trained models\n",
    "print(\"Loading models...\")\n",
    "pose_transformer = PoseTransformer(\n",
    "    pose_dim=POSE_FEATURES_DIM,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    num_heads=8,\n",
    "    num_layers=6,\n",
    "    dropout=0.1,\n",
    "    use_decoder=False\n",
    ").to(device)\n",
    "\n",
    "text_cross_transformer = CrossModalTransformer(\n",
    "    pose_dim=EMBEDDING_DIM,\n",
    "    memory_dim=EMBEDDING_DIM,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    num_heads=8,\n",
    "    num_layers=6,\n",
    "    use_decoder=True\n",
    ").to(device)\n",
    "\n",
    "noise_predictor = torch.nn.Linear(EMBEDDING_DIM, POSE_FEATURES_DIM).to(device)\n",
    "\n",
    "# CLIP Model and Tokenizer\n",
    "clip_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    raise FileNotFoundError(f\"Model checkpoint not found at: {MODEL_PATH}\")\n",
    "# weights only to true to stop receiving the warning\n",
    "checkpoint = torch.load(MODEL_PATH, map_location=device, weights_only=True)\n",
    "\n",
    "pose_transformer.load_state_dict(checkpoint[\"pose_transformer\"])\n",
    "text_cross_transformer.load_state_dict(checkpoint[\"text_cross_transformer\"])\n",
    "noise_predictor.load_state_dict(checkpoint[\"noise_predictor\"])\n",
    "print(\"Model weights loaded successfully.\")\n",
    "\n",
    "pose_transformer.eval()\n",
    "text_cross_transformer.eval()\n",
    "noise_predictor.eval()\n",
    "clip_model.eval()\n",
    "\n",
    "noise_scheduler = NoiseScheduler(timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def infer(text_input, seq_length=60, batch_size=1):\n",
    "\n",
    "    # Tokenize text input and get text embeddings\n",
    "    encoded_texts = clip_tokenizer(\n",
    "        text_input,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=CLIP_MAX_LENGTH,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    text_inputs = {\n",
    "        \"input_ids\": encoded_texts[\"input_ids\"].to(device),\n",
    "        \"attention_mask\": encoded_texts[\"attention_mask\"].to(device)\n",
    "    }\n",
    "\n",
    "    text_embeddings = clip_model.get_text_features(**text_inputs)\n",
    "    \n",
    "    if text_embeddings.ndim == 2:\n",
    "         text_embeddings = text_embeddings.unsqueeze(1)\n",
    "\n",
    "    # Initialize random noise as the starting pose\n",
    "    pose_shape = (batch_size, seq_length, POSE_FEATURES_DIM)\n",
    "    current_pose = torch.randn(pose_shape, device=device)\n",
    "    pose_mask = torch.ones((batch_size, seq_length), device=device, dtype=torch.float32) # <-- Create mask\n",
    "\n",
    "    print(\"Starting denoising loop...\")\n",
    "\n",
    "    # Perform reverse diffusion (denoising)\n",
    "    for t in reversed(range(noise_scheduler.timesteps)):\n",
    "        timesteps = torch.full((batch_size,), t, device=device, dtype=torch.long)\n",
    "\n",
    "        # Get pose embeddings from transformer\n",
    "        pose_embeddings = pose_transformer(\n",
    "            current_pose,\n",
    "            pose_mask=pose_mask,\n",
    "            timesteps=timesteps\n",
    "        )\n",
    "\n",
    "        # Cross-attention with text embeddings\n",
    "        text_conditioned_embeddings = text_cross_transformer(\n",
    "            pose_embeddings,\n",
    "            text_embeddings,\n",
    "            pose_mask, \n",
    "            memory_mask=None\n",
    "        )\n",
    "\n",
    "        # Predict noise\n",
    "        predicted_noise = noise_predictor(text_conditioned_embeddings)\n",
    "\n",
    "        beta_t = noise_scheduler.betas[t].to(device)\n",
    "        alpha_t = noise_scheduler.alphas[t].to(device)\n",
    "        sqrt_one_minus_alpha_cumprod_t = noise_scheduler.sqrt_one_minus_alphas_cumprod[t].to(device)\n",
    "        sqrt_alpha_t = torch.sqrt(alpha_t) \n",
    "\n",
    "        # Calculate the term multiplying the predicted noise\n",
    "        noise_coeff = beta_t / sqrt_one_minus_alpha_cumprod_t\n",
    "\n",
    "        # Calculate the mean of x_{t-1}\n",
    "        mean_x_t_minus_1 = (1.0 / sqrt_alpha_t) * (current_pose - noise_coeff * predicted_noise)\n",
    "\n",
    "        # Add noise (variance term) - except for the last step (t=0)\n",
    "        if t > 0:\n",
    "            variance = beta_t # Use beta_t for variance (sigma_t = sqrt(beta_t))\n",
    "            std_dev = torch.sqrt(variance)\n",
    "            noise = torch.randn_like(current_pose)\n",
    "            current_pose = mean_x_t_minus_1 + std_dev * noise # Update current_pose to x_{t-1}\n",
    "        else:\n",
    "            current_pose = mean_x_t_minus_1\n",
    "\n",
    "    return current_pose.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting denoising loop...\n",
      "(80, 263)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text_prompt = \"A person walking in a straight line.\"\n",
    "generated_poses = infer(text_prompt, seq_length=60, batch_size=1)\n",
    "pose_single = generated_poses[0].numpy()\n",
    "print(pose_single.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.animation import FuncAnimation, FFMpegFileWriter\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "\n",
    "def plot_3d_motion(save_path, kinematic_tree, joints, title, figsize=(10, 10), fps=120, radius=4):\n",
    "    matplotlib.use('Agg')\n",
    "\n",
    "    title_sp = title.split(' ')\n",
    "    if len(title_sp) > 10:\n",
    "        title = '\\n'.join([' '.join(title_sp[:10]), ' '.join(title_sp[10:])])\n",
    "    def init():\n",
    "        ax.set_xlim3d([-radius / 2, radius / 2])\n",
    "        ax.set_ylim3d([0, radius])\n",
    "        ax.set_zlim3d([0, radius])\n",
    "        # print(title)\n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        ax.grid(b=False)\n",
    "\n",
    "    def plot_xzPlane(minx, maxx, miny, minz, maxz):\n",
    "        ## Plot a plane XZ\n",
    "        verts = [\n",
    "            [minx, miny, minz],\n",
    "            [minx, miny, maxz],\n",
    "            [maxx, miny, maxz],\n",
    "            [maxx, miny, minz]\n",
    "        ]\n",
    "        xz_plane = Poly3DCollection([verts])\n",
    "        xz_plane.set_facecolor((0.5, 0.5, 0.5, 0.5))\n",
    "        ax.add_collection3d(xz_plane)\n",
    "\n",
    "    #         return ax\n",
    "\n",
    "    # (seq_len, joints_num, 3)\n",
    "    data = joints.copy().reshape(len(joints), -1, 3)\n",
    "    # fig = plt.figure(figsize=figsize)\n",
    "    # ax = p3.Axes3D(fig)\n",
    "    fig, ax = plt.subplots(subplot_kw={'projection': '3d'}, figsize=figsize)  # Use subplots to create a 3D axis\n",
    "\n",
    "    init()\n",
    "    MINS = data.min(axis=0).min(axis=0)\n",
    "    MAXS = data.max(axis=0).max(axis=0)\n",
    "    colors = ['red', 'blue', 'black', 'red', 'blue',  \n",
    "              'darkblue', 'darkblue', 'darkblue', 'darkblue', 'darkblue',\n",
    "             'darkred', 'darkred','darkred','darkred','darkred']\n",
    "    frame_number = data.shape[0]\n",
    "    #     print(data.shape)\n",
    "\n",
    "    height_offset = MINS[1]\n",
    "    data[:, :, 1] -= height_offset\n",
    "    trajec = data[:, 0, [0, 2]]\n",
    "    \n",
    "    data[..., 0] -= data[:, 0:1, 0]\n",
    "    data[..., 2] -= data[:, 0:1, 2]\n",
    "\n",
    "    #     print(trajec.shape)\n",
    "\n",
    "    def update(index):\n",
    "        #         print(index)\n",
    "        # ax.lines = []\n",
    "        # ax.collections = []\n",
    "        ax.cla()\n",
    "        ax.view_init(elev=120, azim=-90)\n",
    "        ax.dist = 7.5\n",
    "        #         ax =\n",
    "        plot_xzPlane(MINS[0]-trajec[index, 0], MAXS[0]-trajec[index, 0], 0, MINS[2]-trajec[index, 1], MAXS[2]-trajec[index, 1])\n",
    "#         ax.scatter(data[index, :22, 0], data[index, :22, 1], data[index, :22, 2], color='black', s=3)\n",
    "        \n",
    "        if index > 1:\n",
    "            ax.plot3D(trajec[:index, 0]-trajec[index, 0], np.zeros_like(trajec[:index, 0]), trajec[:index, 1]-trajec[index, 1], linewidth=1.0,\n",
    "                      color='blue')\n",
    "        #             ax = plot_xzPlane(ax, MINS[0], MAXS[0], 0, MINS[2], MAXS[2])\n",
    "        \n",
    "        \n",
    "        for i, (chain, color) in enumerate(zip(kinematic_tree, colors)):\n",
    "#             print(color)\n",
    "            if i < 5:\n",
    "                linewidth = 4.0\n",
    "            else:\n",
    "                linewidth = 2.0\n",
    "            ax.plot3D(data[index, chain, 0], data[index, chain, 1], data[index, chain, 2], linewidth=linewidth, color=color)\n",
    "        #         print(trajec[:index, 0].shape)\n",
    "\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_zticklabels([])\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=frame_number, interval=1000/fps, repeat=False)\n",
    "\n",
    "    ani.save(save_path, fps=fps)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from utils.utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "joints_num = 22\n",
    "kinematic_chain = [[0, 2, 5, 8, 11], [0, 1, 4, 7, 10], [0, 3, 6, 9, 12, 15], [9, 14, 17, 19, 21], [9, 13, 16, 18, 20]]\n",
    "# print(example_data_ml3d[0:5])\n",
    "\n",
    "joint = recover_from_ric(torch.from_numpy(pose_single).float(), joints_num).numpy()\n",
    "joint = motion_temporal_filter(joint, sigma=1)\n",
    "\n",
    "trajectory = joint[:, 0, :]\n",
    "x = trajectory[:, 0]\n",
    "z = trajectory[:, 2]\n",
    "n = len(x)\n",
    "plot_3d_motion(\"output/test_ani.mp4\", kinematic_chain, joint, title=\"Testing!\", fps=10)\n",
    "# colors = plt.cm.viridis(np.linspace(0, 1, n))\n",
    "# plt.scatter(x, z, c=colors, s=5)\n",
    "# plt.plot(x, z, linewidth=1)  # Plot X-Z movement\n",
    "# plt.xlabel('X')\n",
    "# plt.ylabel('Z')\n",
    "# plt.title('Root Joint Trajectory')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
