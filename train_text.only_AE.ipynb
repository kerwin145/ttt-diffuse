{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e77b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kelvin C\\.conda\\envs\\machine_learning\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join as pjoin\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence \n",
    "from tqdm import tqdm\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import PoseTransformer\n",
    "# from PoseTransformer import PoseTransformer, CrossModalTransformer\n",
    "from NoiseScheduler import NoiseScheduler\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "mp.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "ACCUMULATION_STEPS = 128\n",
    "EPOCHS = 32\n",
    "SAVE_PATH = \"model_output\"\n",
    "\n",
    "LEARNING_RATE = 2e-4\n",
    "WEIGHT_DECAY = 1e-2\n",
    "PERCENT_TRAIN = 1 \n",
    "\n",
    "EMBEDDING_DIM = 512 \n",
    "POSE_FEATURES_DIM = 263\n",
    "LATENT_DIM = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc6cbdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = \"..\\\\HumanML3D\"\n",
    "train_list = open(pjoin(src_dir, \"train.txt\"), \"r\", encoding=\"utf-8\")\n",
    "val_list = open(pjoin(src_dir, \"val.txt\"), \"r\", encoding=\"utf-8\")\n",
    "test_list = open(pjoin(src_dir, \"test.txt\"), \"r\", encoding=\"utf-8\")\n",
    "\n",
    "clip_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_text_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_text_model.eval()\n",
    "for p in clip_text_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a45ece69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualMLPBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(dim, dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x + self.net(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    # ... (Same definition as in AE training script)\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 128), nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    # ... (Same definition as in AE training script)\n",
    "     def __init__(self, latent_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 256), nn.ReLU(),\n",
    "            nn.Linear(256, output_dim)\n",
    "        )\n",
    "     def forward(self, x): return self.net(x)\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "     def __init__(self, input_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_dim, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, input_dim)\n",
    "     def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstruction = self.decoder(latent)\n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa87a2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_embedding(x, num_frequencies=10):\n",
    "    \"\"\"\n",
    "    Applies Fourier feature encoding to the input tensor.\n",
    "    x: (seq_len, 3) - The trajectory data (X, Y, Z)\n",
    "    num_frequencies: The number of frequency bands\n",
    "    \"\"\"\n",
    "    seq_len, dim = x.shape  # dim = 3 (X, Y, Z)\n",
    "    \n",
    "    # Define frequency bands: log-spaced\n",
    "    freqs = torch.logspace(0.0, np.log10(1000.0), num_frequencies)  # Frequencies in range [1, 1000]\n",
    "    \n",
    "    # Compute sin and cos embeddings for each frequency\n",
    "    x_proj = x.unsqueeze(-1) * freqs  # Shape: (seq_len, 3, num_frequencies)\n",
    "    fourier_features = torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)  # (seq_len, 3, 2*num_frequencies)\n",
    "    \n",
    "    return fourier_features.view(seq_len, -1)  # Flatten to (seq_len, 3 * 2 * num_frequencies)\n",
    "\n",
    "class PoseTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "        (data_dir): contains all the new_joint_vecs\n",
    "        (train_list): the train.txt, as an opened file\n",
    "        (max_len): max length of text descriptions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, src_dir, setting, tokenizer, joint_num, use_percentage = 1.0):\n",
    "        self.src_dir = src_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.joint_num = joint_num \n",
    "        self.pose_features_dim = POSE_FEATURES_DIM\n",
    "\n",
    "        if setting not in [\"train\", \"train_temp\", \"val\", \"test\", \"all\"]:\n",
    "            print(\"Invalid setting. Must be train, val, test, or all.\")\n",
    "            raise\n",
    "\n",
    "        with open(pjoin(src_dir, f\"{setting}.txt\"), \"r\") as f:\n",
    "            self.file_list = [line.strip() for line in f.readlines()]\n",
    "\n",
    "        if 0 < use_percentage < 1.0:\n",
    "            random.shuffle(self.file_list)\n",
    "            num_samples = int(len(self.file_list) * use_percentage)\n",
    "            self.file_list = self.file_list[:num_samples]\n",
    "\n",
    "        # get stats to normalize data later on\n",
    "        self.mean, self.std = None, None\n",
    "        stats = np.load(\"pose_stats.npz\")\n",
    "        self.mean = torch.tensor(stats['mean'], dtype=torch.float32).unsqueeze(0) # Shape (1, pose_features_dim)\n",
    "        self.std = torch.tensor(stats['std'], dtype=torch.float32).unsqueeze(0)   # Shape (1, pose_features_dim)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        file = self.file_list[index]\n",
    "\n",
    "        # Load pose data\n",
    "        pose_path = pjoin(self.src_dir, \"new_joint_vecs\", f\"{file}.npy\")\n",
    "        pose_data = np.load(pose_path)  # Shape: (frames, joints, features)\n",
    "        pose_tensor = torch.tensor(pose_data, dtype=torch.float32)\n",
    "        # normalize\n",
    "        pose_tensor_normalized = (pose_tensor - self.mean) / self.std\n",
    "\n",
    "        # Load text description\n",
    "        text_path = pjoin(self.src_dir, \"texts\", f\"{file}.txt\")\n",
    "\n",
    "        # The descriptions have a extra information such as time stamp and part of speech. I will get rid of that for now to keep things simple.\n",
    "        with open(text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text_descriptions = [\n",
    "                re.split('#', line.strip())[0] for line in f.readlines() if line.strip()\n",
    "            ]\n",
    "\n",
    "        # encoded_texts = self.tokenizer(\n",
    "        #     text_descriptions,\n",
    "        #     truncation=True,\n",
    "        #     padding=\"max_length\",\n",
    "        #     max_length=\n",
    "        # _len,\n",
    "        #     return_tensors=\"pt\"\n",
    "        # )\n",
    "\n",
    "        # Since tokenizer returns tensors in a dictionary, we can access them directly\n",
    "        # input_ids = encoded_texts[\"input_ids\"]\n",
    "        # attention_mask = encoded_texts[\"attention_mask\"]\n",
    "\n",
    "        # world_joints = recover_from_ric(torch.from_numpy(pose_data).float(), self.joint_num)\n",
    "        # root_positions = world_joints[:, 0, :] # get trajectory from root position\n",
    "        # fourier_encoded_traj = fourier_embedding(root_positions)\n",
    "\n",
    "        # Return a list of dictionaries, one per description\n",
    "        return [{\n",
    "            \"pose\": pose_tensor_normalized,\n",
    "            \"text\": text,\n",
    "            # \"text\": input_ids[i],\n",
    "            # \"attention_mask\": attention_mask[i],\n",
    "            # \"trajectory\": fourier_encoded_traj\n",
    "        } for text in text_descriptions]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Pads all pose sequences to the maximum length in the batch.\n",
    "    \"\"\"\n",
    "    flattened_batch = [item for sublist in batch for item in sublist]\n",
    "    \n",
    "    # Extract poses and text descriptions from the batch\n",
    "    poses = [item[\"pose\"] for item in flattened_batch]  # List of pose tensors\n",
    "    padded_poses = pad_sequence(poses, batch_first=True, padding_value=0.0)  # Shape: (batch_size, max_len, ...)\n",
    "    pose_key_padding = (padded_poses.sum(-1) == 0)  \n",
    "\n",
    "    # trajectories = [item[\"trajectory\"] for item in flattened_batch]  # Fourier-encoded trajectories (varied lengths)\n",
    "    # Pad trajectory sequences (pad with zeros to max length in the batch)\n",
    "    # padded_trajectories = pad_sequence(trajectories, batch_first=True, padding_value=0.0)  # Shape: (batch_size, max_traj_len, fourier_dim)\n",
    "\n",
    "    texts = [item[\"text\"] for item in flattened_batch]  # List of tokenized text tensors\n",
    "    tokenized = clip_tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,        # pad to longest in this *batch*\n",
    "        truncation=True,\n",
    "        max_length=None      # optional: drop any hard max\n",
    "    )\n",
    "\n",
    "    input_ids      = tokenized.input_ids\n",
    "    attention_mask = tokenized.attention_mask\n",
    "\n",
    "    return {\n",
    "        \"pose\": padded_poses,\n",
    "        \"pose_mask\": pose_key_padding,\n",
    "        \"text\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        # \"trajectory\": padded_trajectories,\n",
    "        # trajectory_mask not needed as it shares the same dimensions as pose_mask\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c460be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, options, checkpoint_path = None):\n",
    "        self.device = options['device']\n",
    "        self.train_dataloader = options['train_dataloader']\n",
    "        self.eval_dataloader = options['eval_dataloader']\n",
    "        self.clip_text_model = options['clip_text_model']\n",
    "        self.clip_tokenizer = options['clip_tokenizer']\n",
    "        self.pose_transformer = options['pose_transformer']\n",
    "        self.text_cross_transformer = options['text_cross_transformer']\n",
    "        # self.trajectory_cross_transformer = options['trajectory_cross_transformer'].to(self.device)\n",
    "        self.cfg_dropout_prob = options['cfg_dropout_prob']\n",
    "        \n",
    "        self.autoencoder = options['autoencoder']\n",
    "        self.noise_predictor = options['noise_predictor']\n",
    "        \n",
    "        self.accumulation_steps = options['accumulation_steps']\n",
    "        self.pose_encoder = self.autoencoder.encoder\n",
    "\n",
    "        self.noise_scheduler = NoiseScheduler(timesteps=1000)\n",
    "        # send tensors to device\n",
    "        self.noise_scheduler.betas = self.noise_scheduler.betas.to(self.device)\n",
    "        self.noise_scheduler.alphas = self.noise_scheduler.alphas.to(self.device)\n",
    "        self.noise_scheduler.alphas_cumprod = self.noise_scheduler.alphas_cumprod.to(self.device)\n",
    "        self.noise_scheduler.sqrt_alphas_cumprod = self.noise_scheduler.sqrt_alphas_cumprod.to(self.device)\n",
    "        self.noise_scheduler.sqrt_one_minus_alphas_cumprod = self.noise_scheduler.sqrt_one_minus_alphas_cumprod.to(self.device)\n",
    "        self.snr_gamma = 5.0\n",
    "\n",
    "        # clip is left out of the optimizer, as we won't be tuning CLIP model\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            list(self.pose_transformer.parameters()) +\n",
    "            list(self.text_cross_transformer.parameters()) +\n",
    "            # list(self.trajectory_cross_transformer.parameters()) +\n",
    "            list(self.noise_predictor.parameters()),\n",
    "            lr=LEARNING_RATE,\n",
    "            weight_decay=WEIGHT_DECAY\n",
    "        )\n",
    "\n",
    "        num_batches = len(self.train_dataloader) // ACCUMULATION_STEPS\n",
    "        # Add 1 if there are leftover batches that will trigger an optimizer step\n",
    "        if len(self.train_dataloader) % ACCUMULATION_STEPS != 0:\n",
    "            num_batches += 1\n",
    "        training_steps = num_batches * EPOCHS\n",
    "        warmup_steps = int(training_steps * 0.15)\n",
    "        self.lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=warmup_steps,  # Gradually increase LR\n",
    "            num_training_steps=training_steps  # Cosine decay over training\n",
    "        )\n",
    "\n",
    "        self.pose_transformer.train()\n",
    "        self.text_cross_transformer.train()\n",
    "        # self.trajectory_cross_transformer.train()\n",
    "        self.noise_predictor.train()\n",
    "\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "\n",
    "    def save_model(self):\n",
    "        \"\"\"Save only the model weights without optimizer state.\"\"\"\n",
    "        state = {\n",
    "            \"pose_transformer\": self.pose_transformer.state_dict(),\n",
    "            \"text_cross_transformer\": self.text_cross_transformer.state_dict(),\n",
    "            \"noise_predictor\": self.noise_predictor.state_dict(),\n",
    "        }\n",
    "        torch.save(state, self.checkpoint_path)\n",
    "        print(f\"Model weights saved to {self.checkpoint_path}\")\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Load model weights if a checkpoint exists, ignoring optimizer.\"\"\"\n",
    "        if os.path.exists(self.checkpoint_path):\n",
    "            checkpoint = torch.load(self.checkpoint_path, map_location=self.device)\n",
    "            self.pose_transformer.load_state_dict(checkpoint[\"pose_transformer\"])\n",
    "            self.text_cross_transformer.load_state_dict(checkpoint[\"text_cross_transformer\"])\n",
    "            self.noise_predictor.load_state_dict(checkpoint[\"noise_predictor\"])\n",
    "            print(f\"Model weights loaded from {self.checkpoint_path}\")\n",
    "\n",
    "    # def get_lr_lambda(self):\n",
    "    #     return lambda step: min((step + 1) / self.warmup_steps, 1.0)\n",
    "    \n",
    "    def _process_batch(self, batch, compute_loss = True, use_weighted_loss = True):\n",
    "        poses = batch[\"pose\"].to(self.device)\n",
    "        pose_mask = batch[\"pose_mask\"].to(self.device)\n",
    "        texts = batch[\"text\"].to(self.device)\n",
    "        text_mask = batch[\"attention_mask\"].to(self.device)\n",
    "        # trajectory = batch[\"trajectory\"].to(self.device)\n",
    "\n",
    "        # Denoiser setup\n",
    "        batch_size = poses.shape[0]\n",
    "        seq_len = poses.shape[1]\n",
    "\n",
    "        poses_flat = poses.view(-1, POSE_FEATURES_DIM)\n",
    "        with torch.no_grad(): # Ensure no gradients through AE\n",
    "             latent_poses_flat = self.pose_encoder(poses_flat)\n",
    "        latent_poses = latent_poses_flat.view(batch_size, seq_len, LATENT_DIM)\n",
    "\n",
    "        noise = torch.randn_like(latent_poses)\n",
    "        timesteps = self.noise_scheduler.sample_timesteps(batch_size, device=self.device)\n",
    "        noisy_latent_poses = self.noise_scheduler.add_noise(latent_poses, noise, timesteps)\n",
    "\n",
    "        # Get token-wise text embeddings from CLIPTextModel\n",
    "        with torch.no_grad(): # Ensure no gradients computed for CLIP\n",
    "            txt_out = self.clip_text_model(\n",
    "                input_ids=texts,\n",
    "                attention_mask=text_mask,\n",
    "                return_dict=True\n",
    "            )\n",
    "            text_embeddings = txt_out.last_hidden_state\n",
    "\n",
    "        if self.cfg_dropout_prob > 0.0 and self.training:\n",
    "            # Randomly drop condition with probability `cfg_dropout_prob`\n",
    "            drop_mask = torch.rand(batch_size, device=text_embeddings.device) < self.cfg_dropout_prob\n",
    "            drop_mask = drop_mask.view(batch_size, 1, 1)  # (B, 1, 1) for broadcasting\n",
    "\n",
    "            # Replace dropped samples with zeros\n",
    "            text_embeddings = text_embeddings * (~drop_mask)  # Drop entire text embedding if mask is True\n",
    "\n",
    "        pose_embeddings = self.pose_transformer(\n",
    "            noisy_latent_poses,\n",
    "            timesteps=timesteps,\n",
    "            pose_mask=pose_mask\n",
    "        )\n",
    "\n",
    "        # trajectory_conditioned_embeddings = self.trajectory_cross_transformer(\n",
    "        #     pose_embeddings,\n",
    "        #     trajectory,\n",
    "        #     pose_mask=pose_mask,\n",
    "        #     memory_mask=pose_mask\n",
    "        # )\n",
    "\n",
    "        # Cross-attention with text embeddings\n",
    "        text_pad_mask = (text_mask == 0)\n",
    "        text_conditioned_embeddings = self.text_cross_transformer(\n",
    "            pose_embeddings,\n",
    "            text_embeddings,\n",
    "            pose_mask=pose_mask,\n",
    "            memory_mask=text_pad_mask\n",
    "        )\n",
    "        # Predict noise\n",
    "        predicted_noise = self.noise_predictor(text_conditioned_embeddings)\n",
    "\n",
    "        if compute_loss:\n",
    "            mask = ~pose_mask  # Padding mask (B, T)\n",
    "            mask = mask.unsqueeze(-1)  # (B, T, 1)\n",
    "            diff = (predicted_noise - noise) ** 2\n",
    "            masked_diff = diff * mask\n",
    "\n",
    "            if use_weighted_loss:\n",
    "                alphas_cumprod_t = self.noise_scheduler.alphas_cumprod[timesteps]\n",
    "                epsilon = 1e-8\n",
    "                snr = alphas_cumprod_t / (1.0 - alphas_cumprod_t + epsilon) # Shape: (batch_size,)\n",
    "                # Create a tensor of gamma values with the same shape as snr\n",
    "                gamma_tensor = torch.full_like(snr, self.snr_gamma)\n",
    "                # Calculate the minimum between SNR and gamma for each element\n",
    "                snr_clipped_weight = torch.minimum(snr, gamma_tensor) # Shape: (batch_size,)\n",
    "\n",
    "                weight = snr_clipped_weight.view(-1, 1, 1)\n",
    "                weighted_masked_diff = weight * masked_diff\n",
    "                loss = torch.sum(weighted_masked_diff) / mask.sum().clamp(min=1.0) # mask.sum() counts number of valid elements (B*T*F)\n",
    "            else:\n",
    "                loss = torch.sum(masked_diff) / mask.sum().clamp(min=1.0)\n",
    "\n",
    "            return predicted_noise, noise, timesteps, loss\n",
    "        return predicted_noise, noise, timesteps\n",
    "\n",
    "    def train(self):\n",
    "        self.pose_transformer.train()\n",
    "        self.text_cross_transformer.train()\n",
    "        # self.trajectory_cross_transformer.train()\n",
    "        self.noise_predictor.train()\n",
    "        self.training = True\n",
    "\n",
    "        dataloader = self.train_dataloader\n",
    "        total_loss = 0\n",
    "        num_batches = len(dataloader)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        for i, batch in enumerate(tqdm(dataloader, leave=True)):\n",
    "            _, _, _, raw_loss = self._process_batch(batch, use_weighted_loss=True)\n",
    "            loss = raw_loss / self.accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            if (i + 1) % self.accumulation_steps == 0 or (i + 1) == num_batches: #accumulate gradients, and then update.\n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(self.pose_transformer.parameters(), 0.5)\n",
    "                torch.nn.utils.clip_grad_norm_(self.text_cross_transformer.parameters(), 0.5)\n",
    "                torch.nn.utils.clip_grad_norm_(self.noise_predictor.parameters(), 0.5)\n",
    "\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                self.lr_scheduler.step()\n",
    "\n",
    "            total_loss += raw_loss.item()\n",
    "\n",
    "        return total_loss / num_batches\n",
    "\n",
    "    def eval(self):\n",
    "        self.pose_transformer.eval()\n",
    "        self.text_cross_transformer.eval()\n",
    "        # self.trajectory_cross_transformer.eval()\n",
    "        self.noise_predictor.eval()\n",
    "        self.training = False\n",
    "        \n",
    "        dataloader = self.eval_dataloader\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            for batch in tqdm(dataloader, leave=True):\n",
    "                _, _, _, loss = self._process_batch(batch, use_weighted_loss=True)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        return total_loss / len(dataloader)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def init_weights_transformer(module):\n",
    "    \"\"\"\n",
    "    Initialize weights using Transformer-style N(0, 0.02) for Linear & Embedding.\n",
    "    Initialize LayerNorm weights to 1 and biases to 0.\n",
    "    \"\"\"\n",
    "    std_dev = 0.02 # Standard deviation for normal initialization\n",
    "    if isinstance(module, nn.Linear):\n",
    "        # Normal initialization for weights\n",
    "        nn.init.normal_(module.weight, mean=0.0, std=std_dev)\n",
    "        # Zero initialization for biases if they exist\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "        # Normal initialization for embedding weights\n",
    "        nn.init.normal_(module.weight, mean=0.0, std=std_dev)\n",
    "        # Zero out padding embedding if it exists\n",
    "        if module.padding_idx is not None:\n",
    "            with torch.no_grad(): # Ensure this operation isn't tracked by autograd\n",
    "                module.weight[module.padding_idx].fill_(0)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        # Initialize weights (gamma) to 1\n",
    "        nn.init.ones_(module.weight)\n",
    "        # Initialize biases (beta) to 0\n",
    "        nn.init.zeros_(module.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "419052c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Train dataset\n",
      "Loading Eval Dataset\n",
      "Dataset loading done\n",
      "Autoencoder weights loaded successfully.\n",
      "Pose Transformer Parameters: 28916736\n",
      "Text-Cross Transformer Parameters: 38361600\n",
      "Noise Predictor Parameters: 820800\n",
      "Total Trainable Parameters: 68099136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2558/2558 [04:45<00:00,  8.97it/s]\n",
      "  0%|          | 0/183 [00:00<?, ?it/s]c:\\Users\\Kelvin C\\.conda\\envs\\machine_learning\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:502: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\NestedTensorImpl.cpp:180.)\n",
      "  output = torch._nested_tensor_from_mask(\n",
      "100%|██████████| 183/183 [00:07<00:00, 25.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trian loss:  67.63231318140515 Eval loss:  69.4149723574112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2558/2558 [04:45<00:00,  8.96it/s]\n",
      "100%|██████████| 183/183 [00:07<00:00, 25.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trian loss:  66.62419508452338 Eval loss:  65.16565903939836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2558/2558 [04:46<00:00,  8.93it/s]\n",
      "100%|██████████| 183/183 [00:07<00:00, 25.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trian loss:  64.43175084223684 Eval loss:  60.30063433204192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2558/2558 [04:46<00:00,  8.93it/s]\n",
      "100%|██████████| 183/183 [00:07<00:00, 25.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trian loss:  62.095268887593654 Eval loss:  58.14047638314669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2558/2558 [04:46<00:00,  8.93it/s]\n",
      "100%|██████████| 183/183 [00:07<00:00, 25.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trian loss:  58.18235133377438 Eval loss:  54.435082440819244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2558/2558 [04:46<00:00,  8.93it/s]\n",
      "100%|██████████| 183/183 [00:07<00:00, 25.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trian loss:  55.009397723042845 Eval loss:  49.8476558487272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2558/2558 [04:46<00:00,  8.93it/s]\n",
      "100%|██████████| 183/183 [00:07<00:00, 25.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trian loss:  52.58442233031014 Eval loss:  41.79187614149083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2558/2558 [04:45<00:00,  8.97it/s]\n",
      "100%|██████████| 183/183 [00:07<00:00, 25.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trian loss:  49.402852711591805 Eval loss:  37.33945724872944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2558/2558 [04:45<00:00,  8.96it/s]\n",
      "100%|██████████| 183/183 [00:07<00:00, 25.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trian loss:  46.25761971546394 Eval loss:  33.82072605787079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2558/2558 [04:48<00:00,  8.88it/s]\n",
      "100%|██████████| 183/183 [00:07<00:00, 24.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trian loss:  43.46024426480026 Eval loss:  33.542387050357675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2558/2558 [04:47<00:00,  8.89it/s]\n",
      "100%|██████████| 183/183 [00:07<00:00, 25.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trian loss:  41.23181804490332 Eval loss:  32.90505821978459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2558/2558 [04:45<00:00,  8.96it/s]\n",
      "100%|██████████| 183/183 [00:07<00:00, 25.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trian loss:  39.61359235851722 Eval loss:  30.933903619891307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2558/2558 [04:45<00:00,  8.97it/s]\n",
      "100%|██████████| 183/183 [00:07<00:00, 25.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trian loss:  38.36137703585382 Eval loss:  31.6024882272293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2558/2558 [04:45<00:00,  8.96it/s]\n",
      "100%|██████████| 183/183 [00:07<00:00, 25.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trian loss:  36.69253454062042 Eval loss:  27.236725549228854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2558/2558 [04:45<00:00,  8.95it/s]\n",
      "100%|██████████| 183/183 [00:07<00:00, 25.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trian loss:  35.24017686607131 Eval loss:  26.847140551916237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2558/2558 [04:45<00:00,  8.95it/s]\n",
      "100%|██████████| 183/183 [00:07<00:00, 25.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trian loss:  34.1178786090988 Eval loss:  25.38556125124947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2558/2558 [04:45<00:00,  8.96it/s]\n",
      "100%|██████████| 183/183 [00:07<00:00, 25.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trian loss:  32.78353840243173 Eval loss:  26.81474594210015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2558/2558 [04:45<00:00,  8.95it/s]\n",
      "100%|██████████| 183/183 [00:07<00:00, 25.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trian loss:  32.193604522938834 Eval loss:  24.875097995247344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2558/2558 [04:45<00:00,  8.97it/s]\n",
      "100%|██████████| 183/183 [00:07<00:00, 25.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trian loss:  31.526945998698498 Eval loss:  24.638086440784683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2558/2558 [04:45<00:00,  8.96it/s]\n",
      "100%|██████████| 183/183 [00:07<00:00, 25.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trian loss:  31.09122835867474 Eval loss:  24.496959804837168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2558/2558 [04:44<00:00,  8.98it/s]\n",
      "100%|██████████| 183/183 [00:07<00:00, 25.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trian loss:  29.369648043989574 Eval loss:  23.82195739602782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2558/2558 [04:45<00:00,  8.97it/s]\n",
      "100%|██████████| 183/183 [00:07<00:00, 25.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trian loss:  29.083068835819205 Eval loss:  23.80381487888065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2558/2558 [04:44<00:00,  9.00it/s]\n",
      "100%|██████████| 183/183 [00:07<00:00, 25.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trian loss:  28.76593630970037 Eval loss:  23.400585254684824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2558/2558 [04:43<00:00,  9.01it/s]\n",
      "100%|██████████| 183/183 [00:07<00:00, 25.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trian loss:  28.841388711573277 Eval loss:  21.513050856486043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2558/2558 [04:45<00:00,  8.97it/s]\n",
      "100%|██████████| 183/183 [00:07<00:00, 25.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trian loss:  28.479131788994017 Eval loss:  22.29633912492971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2558/2558 [04:45<00:00,  8.97it/s]\n",
      "100%|██████████| 183/183 [00:07<00:00, 25.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trian loss:  28.837281724174474 Eval loss:  21.997825536897274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2558/2558 [04:45<00:00,  8.96it/s]\n",
      "100%|██████████| 183/183 [00:07<00:00, 25.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trian loss:  28.176687142875068 Eval loss:  24.181013937204913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 481/2558 [00:53<03:51,  8.99it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: misaligned address\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 115\u001b[0m\n\u001b[0;32m    112\u001b[0m best_model_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m--> 115\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m    117\u001b[0m     eval_loss \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[1;32mIn[5], line 180\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    178\u001b[0m _, _, _, raw_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_batch(batch, use_weighted_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    179\u001b[0m loss \u001b[38;5;241m=\u001b[39m raw_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulation_steps\n\u001b[1;32m--> 180\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m num_batches: \u001b[38;5;66;03m#accumulate gradients, and then update.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Gradient clipping\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpose_transformer\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.5\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Kelvin C\\.conda\\envs\\machine_learning\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kelvin C\\.conda\\envs\\machine_learning\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kelvin C\\.conda\\envs\\machine_learning\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: misaligned address\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)  # Python random module\n",
    "np.random.seed(SEED)  # NumPy random\n",
    "torch.manual_seed(SEED)  # PyTorch random\n",
    "\n",
    "# importlib.reload(PoseTransformer)\n",
    "\n",
    "POSE_TRANSFORMER_LAYERS = 8\n",
    "CROSS_TRANSFORMER_LAYERS = 8\n",
    "CFG_DROPOUT_RATE = 0.1\n",
    "\n",
    "# 5.1.0 uses SNR weighting\n",
    "# 5.1.1 changes cross transformer layers from 6 to 4\n",
    "# 6 introducd AE\n",
    "# 7 introduced CFG \n",
    "# 8 introduce RMSE\n",
    "\n",
    "checkpoint_path = f\"model_saves/v7.0.0_TT_datapercent{PERCENT_TRAIN}_lr{LEARNING_RATE}_WD{WEIGHT_DECAY}_P{POSE_TRANSFORMER_LAYERS}C{CROSS_TRANSFORMER_LAYERS}_CFG{CFG_DROPOUT_RATE}_ACC{ACCUMULATION_STEPS}\"\n",
    "\n",
    "print(\"Loading Train dataset\")\n",
    "trainDataset = PoseTextDataset(src_dir=src_dir,setting=\"train\",tokenizer=clip_tokenizer, joint_num=22, use_percentage=PERCENT_TRAIN)\n",
    "trainDataLoader = DataLoader(trainDataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=0,collate_fn=collate_fn)\n",
    "# persistent_workers=True,\n",
    "# multiprocessing_context=\"spawn\"\n",
    "print(\"Loading Eval Dataset\")\n",
    "evalDataset = PoseTextDataset(src_dir=src_dir,setting=\"val\",tokenizer=clip_tokenizer, joint_num=22, use_percentage=1)\n",
    "evalDataLoader = DataLoader(evalDataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=0,collate_fn=collate_fn)\n",
    "print(\"Dataset loading done\")\n",
    "pose_transformer = PoseTransformer.PoseTransformer(\n",
    "    pose_dim=LATENT_DIM,\n",
    "    embedding_dim=EMBEDDING_DIM,  # Match CLIP's embedding dimension\n",
    "    num_heads=8,  # Number of attention heads\n",
    "    num_layers=POSE_TRANSFORMER_LAYERS,  # Number of transformer layers\n",
    "    dropout=CFG_DROPOUT_RATE,  # Dropout probability\n",
    "    use_decoder=False\n",
    ").to(device)\n",
    "\n",
    "text_cross_transformer = PoseTransformer.CrossModalTransformer(\n",
    "    pose_dim = EMBEDDING_DIM,\n",
    "    memory_dim= EMBEDDING_DIM,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    num_heads=8,\n",
    "    num_layers=CROSS_TRANSFORMER_LAYERS,\n",
    "    use_decoder=True\n",
    ").to(device)\n",
    "\n",
    "noise_predictor = nn.Sequential(\n",
    "    ResidualMLPBlock(EMBEDDING_DIM),\n",
    "    nn.Linear(EMBEDDING_DIM, EMBEDDING_DIM),\n",
    "    nn.SiLU(),\n",
    "    nn.Linear(EMBEDDING_DIM, LATENT_DIM),\n",
    ").to(device)\n",
    "\n",
    "autoencoder = Autoencoder(POSE_FEATURES_DIM, LATENT_DIM).to(device)\n",
    "ae_checkpoint = torch.load(\"model_saves\\\\autoencoder\\\\pose_ae_best.pth\", map_location=device, weights_only=True)\n",
    "# Verify dimensions match\n",
    "if ae_checkpoint['input_dim'] != POSE_FEATURES_DIM:\n",
    "        raise ValueError(f\"AE Checkpoint input_dim ({ae_checkpoint['input_dim']}) doesn't match expected ({POSE_FEATURES_DIM})\")\n",
    "if ae_checkpoint['latent_dim'] != LATENT_DIM:\n",
    "        raise ValueError(f\"AE Checkpoint latent_dim ({ae_checkpoint['latent_dim']}) doesn't match expected ({LATENT_DIM})\")\n",
    "\n",
    "autoencoder.load_state_dict(ae_checkpoint['model_state_dict'])\n",
    "print(\"Autoencoder weights loaded successfully.\")\n",
    "autoencoder.eval()\n",
    "for param in autoencoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "pose_transformer.apply(init_weights_transformer)\n",
    "text_cross_transformer.apply(init_weights_transformer)\n",
    "noise_predictor.apply(init_weights_transformer) # Apply standard init first\n",
    "\n",
    "# trajectory_cross_transformer = CrossModalTransformer(\n",
    "#     pose_dim = EMBEDDING_DIM,\n",
    "#     memory_dim= 60,\n",
    "#     embedding_dim=EMBEDDING_DIM,\n",
    "#     num_heads=8,\n",
    "#     num_layers=6,\n",
    "#     use_decoder=True\n",
    "# )\n",
    "\n",
    "trainer = Trainer({\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"train_dataloader\": trainDataLoader,\n",
    "    \"eval_dataloader\": evalDataLoader,\n",
    "    \"clip_text_model\": clip_text_model,\n",
    "    \"clip_tokenizer\": clip_tokenizer,\n",
    "    \"pose_transformer\": pose_transformer,\n",
    "    \"text_cross_transformer\": text_cross_transformer,\n",
    "    # \"trajectory_cross_transformer\": trajectory_cross_transformer,\n",
    "    \"cfg_dropout_prob\": 0.1,\n",
    "    \"autoencoder\": autoencoder,\n",
    "    \"noise_predictor\": noise_predictor,\n",
    "    \"pose_features_dim\": POSE_FEATURES_DIM,\n",
    "    \"embedding_dim\": EMBEDDING_DIM,\n",
    "    \"accumulation_steps\": ACCUMULATION_STEPS,\n",
    "}, checkpoint_path=checkpoint_path)\n",
    "# Print for each model\n",
    "print(f\"Pose Transformer Parameters: {count_parameters(trainer.pose_transformer)}\")\n",
    "print(f\"Text-Cross Transformer Parameters: {count_parameters(trainer.text_cross_transformer)}\")\n",
    "print(f\"Noise Predictor Parameters: {count_parameters(trainer.noise_predictor)}\")\n",
    "\n",
    "# Total trainable parameters\n",
    "total_params = sum([\n",
    "    count_parameters(trainer.pose_transformer),\n",
    "    count_parameters(trainer.text_cross_transformer),\n",
    "    count_parameters(trainer.noise_predictor),\n",
    "])\n",
    "print(f\"Total Trainable Parameters: {total_params}\")\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "best_eval_loss = float(\"inf\")\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = trainer.train()\n",
    "    train_losses.append(train_loss)\n",
    "    eval_loss = trainer.eval()\n",
    "    eval_losses.append(eval_loss)\n",
    "    print(\"Trian loss: \", train_loss, \"Eval loss: \", eval_loss)\n",
    "    if eval_loss < best_eval_loss:\n",
    "        best_eval_loss = eval_loss\n",
    "        best_model_state = {\n",
    "            \"pose_transformer\": trainer.pose_transformer.state_dict(),\n",
    "            \"text_cross_transformer\": trainer.text_cross_transformer.state_dict(),\n",
    "            \"noise_predictor\": trainer.noise_predictor.state_dict(),\n",
    "        }\n",
    "\n",
    "if best_model_state:\n",
    "    torch.save(best_model_state, checkpoint_path)\n",
    "    print(f\"Best model saved with eval loss {best_eval_loss:.4f} at {checkpoint_path}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, EPOCHS + 1), train_losses, marker='o', linestyle='-', color='b', label=\"Train Loss\")\n",
    "plt.plot(range(1, EPOCHS + 1), eval_losses, marker='s', linestyle='--', color='r', label=\"Eval Loss\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.title(\"Training & Evaluation Loss Curve\")\n",
    "plt.legend()  # Show legend to differentiate train and eval losses\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
